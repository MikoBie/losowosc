---
title: "Appendix A"
description: |
    Appendix A.
author:
  - name: Mikołaj Biesaga and Szymon Talaga
    affiliation: The Robert Zajonc Institute for Social Studies
    affiliation_url: www.iss.uw.edu.pl/en/
date: "`r Sys.Date()`"
output: radix::radix_article
---

## 2. Method

```{r setup_env, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 8, fig.asp = 1)
```

```{r setup}
## Wczytaj pakiety
library(magrittr)
library(tidyverse)
library(reticulate)
library(lattice)
library(zoo)
library(lme4)
library(lmerTest)
library(MuMIn)
library(acss)
library(haven)
library(nlme)
library(sjPlot)
library(semPlot)
library(kableExtra)
library(lavaan)
## Ustaw temat ggplot()
theme_set(theme_classic())
## Środowisko Conda
use_condaenv("bdm")
```

### 2.1. Study 1

In the first study, participants were tested individually in sessions that lasted about 15 minutes. They were simply asked to produce a 300 elements binary series. To gather the responses from the participants we used a specially designed computer tool which is available on GitHub (https://github.com/MikoBie/Survey).


```{r load_and_prepare_data}
## Wczytanie danych z wyniki_new.csv
data <- read_delim("dane/wyniki_new.csv", delim = ";") %>%
    rename_at(vars(matches("^\\d")), ~str_c("d", .x)) %>%
    mutate_at(vars(matches("^d\\d")), as.integer) %>%
    rename(id = X)

## Zamiana na format long
data_long <- gather(data, key = "Index", value = "Bit", matches("^d\\d")) %>%
    filter(!is.na(Bit)) %>%
    arrange(id) %>%
    group_by(id) %>% 
    summarize(seq = list(Bit)) %>%
    ungroup 
``` 

#### 2.1.1. Procedure and Design

The experiment followed a 2 x 3 factorial design, including two between-subjects variables: the mathematical experience and the task definition condition. We recruited as participants two groups of students who either studied Psychology or Chemistry at the University of Warsaw. We assumed that students who chose as their major Psychology have less experience with a concept like a randomness while students from the Chemistry Department are more familiar with it. We based our assumption on the number of obligatory courses students had to take in both departments. For Chemistry it was nearly 200 hours of subjects like Math, Physics, and Statistics during the first three years, while for Psychology it was just 90 hours of Statistics during 5 years. In both groups, participants were at random assigned to one of three experimental conditions: 1) the No Instruction Condition, 2) the Coin Tossing Condition, and 3) the Stock Market Condition. In all three conditions, we used a specially designed computer tool. It displayed every two seconds a red square. In the No Instruction Condition participants were simply asked to press in random order one of the two specially marked keys whenever they see a red square. In the Coin Tossing Condition, participants were instructed to imagine tossing a fair coin whenever they see a red square and press either key marked as tail or head. In the Stock Market Condition, participants were asked to imagine a stock market chart and to assume that price fluctuation is generated by a random process. They were instructed to try to predict whether the prices will go up or down and to press either key marked as an arrow up or down. 

#### 2.1.2. Participants

The participants were students from the Psychology Department and the Chemistry Department at the University of Warsaw. A total of $183$ subjects ($129$ females), aged from $18$ to $30$ ($M = 21.54,\ SD = 2.12$), were randomly assigned to one of the experimental conditions. The procedure was approved by the ethics committee of the Robert Zajonc Institute for Social Studies at the University of Warsaw. All participants gave informed consent before taking part in the study. 

#### 2.1.3. Data manipulation

Although the participants were instructed to only press relevant keys when they saw a red square some people pressed it more often and others less frequent than 300 times. Therefore, the length of the series varies between subjects from 218 to 1016 elements ($Median = 300$). However, we used only observations with indexes smaller than 313, thus we cut off the last 10% of observations. There were only a couple of people who produced a longer series.

We used „pybdm” library in Python to compute algorithmic complexity of each series. It is an implementation of Block Decomposition Method which allows extending the power of Coding Theorem Method on longer strings. All other analyses were performed using R (R Core Team, 2019). We used packages „dplyr” (version 0.8.1), „magrittr” (version 1.5),  lme4 (version 1.1), and ggplot2 (version 3.2.0) for data manipulation, processing, visualization, and mixed models computation. 

For each participant, we computed the overall value of the series complexity and the rolling algorithmic complexity. The latter was based on the computation of algorithmic complexity for a sliding window of length from 5 to 9. That is because the length of the working memory capacity is $7\pm2$ bits of information. We calculated algorithmic complexity for all the lengths separately and performed the analysis for all the lengths.

```{python prepare_strings}
import numpy as np
import pandas as pd
from bdm import BDMRecursive as BDM

def window_bdm(seq, bdm, k=7):
    return np.array([ bdm.bdm(seq[i:(i+k)]) for i in range(len(seq) - k) ])
    
data = r.data_long
data.id = data.id.astype(int)
data.seq = data.seq.apply(lambda x: np.array(x, dtype=int))

bdm = BDM(ndim=1, min_length=7)
seq7 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm))})

bdm = BDM(ndim=1, min_length=5)
seq5 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 5))})

bdm = BDM(ndim=1, min_length=9)
seq9 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 9))})
                     
bdm = BDM(ndim=1, min_length=6)
seq6 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 6))})

bdm = BDM(ndim=1, min_length=8)
seq8 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 8))})
                     
bdm = BDM(ndim=1, min_length=15)
seq15 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 15))})
                     


# seq5 = seq5.loc[seq5.cmx >= seq5.cmx.quantile(.05), :]
# seq7 = seq7.loc[seq7.cmx >= seq7.cmx.quantile(.05), :]
# seq9 = seq9.loc[seq9.cmx >= seq9.cmx.quantile(.05), :]
# seq6 = seq11.loc[seq6.cmx >= seq6.cmx.quantile(.05), :]
# seq8 = seq13.loc[seq8.cmx >= seq8.cmx.quantile(.05), :]
# seq15 = seq15.loc[seq15.cmx >= seq15.cmx.quantile(.05), :]
```


```{r data_processing}
seq7 <- tbl_df(py$seq7)
seq5 <- tbl_df(py$seq5)
seq9 <- tbl_df(py$seq9)
seq6 <- tbl_df(py$seq6)
seq8 <- tbl_df(py$seq8)
seq15 <- tbl_df(py$seq15)


data <- select(data, -matches("^d\\d")) %>%
    filter(id %in% seq7$id) %>%
    left_join(select(seq7, id, cmx), by = "id")

seq7 <- seq7 %>%
    unnest() %>%
    group_by(id) %>%
    mutate(idx = 1:n(),
           cmx_r = rollmean(cmx_w, k = 7, align = "center", na.pad = TRUE)) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    mutate(Condition = fct_relevel(as.factor(Condition), "zero")) %>%
    filter(idx < 313) %>%
    group_by(id) %>%
    mutate(cmx_wc = mean(cmx_w) - cmx_w)

seq5 <- seq5 %>%
    unnest() %>%
    group_by(id) %>%
    mutate(idx = 1:n(),
           cmx_r = rollmean(cmx_w, k = 5, align = "center", na.pad = TRUE)) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    mutate(Condition = fct_relevel(as.factor(Condition), "zero")) %>%
    filter(idx < 313)  %>%
    mutate(cmx_wc = mean(cmx_w) - cmx_w)

seq9 <- seq9 %>%
    unnest() %>%
    group_by(id) %>%
    mutate(idx = 1:n(),
           cmx_r = rollmean(cmx_w, k = 9, align = "center", na.pad = TRUE)) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    mutate(Condition = fct_relevel(as.factor(Condition), "zero")) %>%
    filter(idx < 313)  %>%
    mutate(cmx_wc = mean(cmx_w) - cmx_w)

seq6 <- seq6 %>%
    unnest() %>%
    group_by(id) %>%
    mutate(idx = 1:n(),
           cmx_r = rollmean(cmx_w, k = 6, align = "center", na.pad = TRUE)) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    mutate(Condition = fct_relevel(as.factor(Condition), "zero")) %>%
    filter(idx < 313)  %>%
    mutate(cmx_wc = mean(cmx_w) - cmx_w)


seq8 <- seq8 %>%
    unnest() %>%
    group_by(id) %>%
    mutate(idx = 1:n(),
           cmx_r = rollmean(cmx_w, k = 8, align = "center", na.pad = TRUE)) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    mutate(Condition = fct_relevel(as.factor(Condition), "zero")) %>%
    filter(idx < 313)  %>%
    mutate(cmx_wc = mean(cmx_w) - cmx_w)


seq15 <- seq15 %>%
    unnest() %>%
    group_by(id) %>%
    mutate(idx = 1:n(),
           cmx_r = rollmean(cmx_w, k = 15, align = "center", na.pad = TRUE)) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    mutate(Condition = fct_relevel(as.factor(Condition), "zero")) %>%
    filter(idx < 313)  %>%
    mutate(cmx_wc = mean(cmx_w) - cmx_w)

```

```{r model5}
# ## Model selection
# model5 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
#              random = ~idx|id,
#              data = seq5)
# model5.4 <- update(model5,correlation = corARMA(form = ~idx|id, p = 1))
# anova(model5, model5.4)
# model5.5 <- update(model5.4,random = ~1|id)
# anova(model5.5, model5.4)
# model5.6 <- update(model5.5, random = ~idx-1|id )
# anova(model5.6,model5.4)
# #model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
# #model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
# #anova(model3, model4, model5, model6, model7)
# 
# ## Errors diagnostics
# par(mfrow = c(1,2))
# plot(model5.4)
# #qqnorm(model2,~ranef(., level = 1))
# qqnorm(model5.4, abline = c(0,1))
# plot(model5.4, resid(., type = 'normalized') ~ fitted(.))
# plot(model5.4,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
# boxplot(resid(model5.4, type = 'normalized') ~ seq5$Condition)
# boxplot(resid(model5.4, type = 'normalized') ~ seq5$Faculty)
# 
# ## Autocorrelation Diagnostics
# acf(resid(model5.4, type = 'normalized'))
# pacf(resid(model5.4, type = 'normalized'))
# acf(resid(model5.4))
# pacf(resid(model5.4))
# 
# ## Summary
# r.squaredGLMM(model5.4)
# summary(model5.4)
# seq5 <- seq5 %>%
#   mutate(predict = predict(model5.4))
```


```{r model6}
# ## Model specification
# model6 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
#              random = ~idx|id,
#              data = seq6)
# model6.4 <- update(model6,correlation = corARMA(form = ~idx|id, p = 1))
# anova(model6, model6.4)
# model6.5 <- update(model6.4,random = ~1|id)
# anova(model6.5, model6.4)
# model6.6 <- update(model6.5, random = ~idx-1|id )
# anova(model6.6,model6.4)
# #model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
# #model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
# #anova(model3, model4, model5, model6, model7)
# 
# ## Error Diagnostics
# par(mfrow = c(1,2))
# plot(model6.6)
# #qqnorm(model2,~ranef(., level = 1))
# qqnorm(model6.6, abline = c(0,1))
# plot(model6.6, resid(., type = 'normalized') ~ fitted(.), type = c('p', 'smooth'))
# boxplot(resid(model6.6, type = 'pearson') ~ seq6$Condition)
# boxplot(resid(model6.6, type = 'pearson') ~ seq6$Faculty)
# 
# ## Autocorrelation Diagnostics
# acf(resid(model6.6, type = 'normalized'))
# pacf(resid(model6.6, type = 'normalized'))
# acf(resid(model6.6))
# pacf(resid(model6.6))
# 
# ## Summary
# r.squaredGLMM(model6.6)
# summary(model6.6)
# seq6 <- seq6 %>%
#   mutate(predict = predict(model6.4))
```

```{r model7}
# ## Model Selection
# model7 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
#              random = ~idx|id,
#              data = seq7)
# model7.4 <- update(model7,correlation = corARMA(form = ~idx|id, p = 1))
# anova(model7, model7.4)
# model7.5 <- update(model7.4,random = ~1|id)
# anova(model7.5, model7.4)
# model7.6 <- update(model7.4, random = ~idx-1|id )
# anova(model7.6,model7.4)
# #model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
# #model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
# #anova(model3, model4, model5, model6, model7)
# 
# ## Error Diagnostics
# par(mfrow = c(1,2))
# plot(model7.5)
# #qqnorm(model2,~ranef(., level = 1))
# qqnorm(model7.5, abline = c(0,1))
# plot(model7.5, resid(., type = 'normalized') ~ fitted(.))
# plot(model7.5,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
# boxplot(resid(model7.5, type = 'normalized') ~ seq7$Condition)
# boxplot(resid(model7.5, type = 'normalized') ~ seq7$Faculty)
# 
# ## Autocorrelation Diagnostics
# acf(resid(model7.5, type = 'normalized'))
# pacf(resid(model7.5, type = 'normalized'))
# acf(resid(model7.5))
# pacf(resid(model7.5))
# 
# ## Summary
# r.squaredGLMM(model7.5)
# summary(model7.5)
# seq7 <- seq7 %>%
#   ungroup() %>%
#   mutate(predict = predict(model7.5))
```

```{r model8}
## Model Selection
model8 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
             random = ~idx|id,
             data = seq8)
model8.4 <- update(model8,correlation = corARMA(form = ~idx|id, p = 1))
anova(model8, model8.4)
model8.5 <- update(model8.4,random = ~1|id)
# anova(model8.5, model8.4)
# model8.6 <- update(model8.5, random = ~idx-1|id )
# anova(model8.6,model8.4)
# #model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
# #model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
# #anova(model3, model4, model5, model6, model7)
# model8.7 <- update(model8.5, correlation = corARMA(form = ~1|id, p = 1))
# 
# ## Error Diagnostics
# par(mfrow = c(1,2))
# plot(model8.5)
# #qqnorm(model2,~ranef(., level = 1))
# qqnorm(model8.5, abline = c(0,1))
# plot(model8.5, resid(., type = 'normalized') ~ fitted(.), type('p', 'smooth'))
# plot(model8.5,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
# boxplot(resid(model8.5, type = 'normalized') ~ seq8$Condition)
# boxplot(resid(model8.5, type = 'normalized') ~ seq8$Faculty)
# 
# ## Autocorrealtion Diagnostics
# acf(resid(model8.5, type = 'normalized'))
# pacf(resid(model8.5, type = 'normalized'))
# acf(resid(model8.7))
# pacf(resid(model8.7))
# 
# ## Summary
# r.squaredGLMM(model8.5)
# summary(model8.5)
# seq8 <- seq8 %>%
#   mutate(predict = predict(model8.5))
```

```{r model9}
# ## Model Selection
# model9 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
#              random = ~idx|id,
#              data = seq9)
# model9.4 <- update(model9,correlation = corARMA(form = ~idx|id, p = 1))
# anova(model9, model9.4)
# model9.5 <- update(model9.4,random = ~1|id)
# anova(model9.5, model9.4)
# model9.6 <- update(model9.5, random = ~idx-1|id )
# anova(model9.6,model9.4)
# #model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
# #model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
# #anova(model3, model4, model5, model6, model7)
# 
# ## Error Diagonostics
# par(mfrow = c(1,2))
# plot(model9.5)
# #qqnorm(model2,~ranef(., level = 1))
# qqnorm(model9.5, abline = c(0,1))
# plot(model9.5, resid(., type = 'normalized') ~ fitted(.))
# plot(model9.5,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
# boxplot(resid(model9.5, type = 'normalized') ~ seq9$Condition)
# boxplot(resid(model9.5, type = 'normalized') ~ seq9$Faculty)
# 
# ## Autocorrelation Diagnostics
# acf(resid(model9.5, type = 'normalized'))
# pacf(resid(model9.5, type = 'normalized'))
# acf(resid(model9.5))
# pacf(resid(model9.5))
# 
# ## Summary
# r.squaredGLMM(model9.5)
# summary(model9.5)
# seq9 <- seq9 %>%
#   mutate(predict = predict(model9.5))
```

```{r model_comparison}
## I am not sure if we can do that this way.
## AIC(model5.4,model6.6, model7.5, model8.5, model9.5)

```



#### 2.1.4. Results

```{r mean_difference_psychology_chemistry}
## Function to compute a naive permutation test. It takes data (obviously), number of iterations, and grouping variable. It returns series of differences of mean and differences of sd.
permutaion_test <- function(data = data,R = 100000,group_var = 'Faculty'){
  
  results_mean <- vector(mode = 'numeric', length = R)
  results_sd <- vector(mode = 'numeric', length = R)
  
  group_n <- data %>%
    group_by_at(group_var) %>%
    summarise(n = n()) %$%
    {n[1]}
  
  total_n <- data %>%
    nrow
  
  set.seed(8710)
  
  for(i in c(1:R)){
    index <- sample(total_n, group_n, replace = FALSE)
    results_mean[i] <- mean(data$cmx[index]) - mean(data$cmx[-index])
    results_sd[i] <-  sd(data$cmx[-index]) - sd(data$cmx[index])
  }
  results <- tibble(diff_means = results_mean,
                    diff_sd = results_sd)
  return(results)
}
## Function which computes p for permutation test. It takes a data table with column mean with observed difference in means and sd with observed difference in sd. As a second argument it takes result of permutation test. It returns a data frame with p values for difference in means and sd.
p_value <- function(observed_difference, distribution){
  nperm <- nrow(distribution)
  mean_diff <- observed_difference$mean[1] - observed_difference$mean[2]
  sd_diff <- observed_difference$sd[2] - observed_difference$sd[1]
  mean_p <- (sum(distribution$diff_means >= mean_diff) +1) / (nperm +1)
  sd_p <- (sum(distribution$diff_sd >= (mean_diff)) +1) / (nperm +1)
  results <- tibble(mean = mean_p, sd = sd_p)
  return(results)
}

## Chemistry - Psychology
permutation_faculty <- permutaion_test(data = data, group_var = 'Faculty')
observed_difference <- data %>%
    group_by(Faculty) %>%
    summarise(mean = mean(cmx),
              sd = sd(cmx))
p_value(observed_difference = observed_difference, distribution = permutation_faculty)

## Coin - Zero
permutation_coin_zero <- data %>%
  filter(Condition != 'stock') %>%
  permutaion_test(data = ., group_var = 'Condition')
observed_difference <- data %>%
  filter(Condition != 'stock') %>%
  group_by(Condition) %>%
  summarise(mean = mean(cmx),
            sd = sd(cmx))
p_value(observed_difference = observed_difference, distribution = permutation_coin_zero)

## Coin - Stock
permutation_coin_stock <- data %>%
  filter(Condition != 'zero') %>%
  permutaion_test(data = ., group_var = 'Condition')
observed_difference <- data %>%
  filter(Condition != 'zero') %>%
  group_by(Condition) %>%
  summarise(mean = mean(cmx),
            sd = sd(cmx))
p_value(observed_difference = observed_difference, distribution = permutation_coin_stock)

## Stock - zero
permutation_stock_zero <- data %>%
  filter(Condition != 'coin') %>%
  permutaion_test(data = ., group_var = 'Condition')
observed_difference <- data %>%
  filter(Condition != 'coin') %>%
  group_by(Condition) %>%
  summarise(mean = mean(cmx),
            sd = sd(cmx))
p_value(observed_difference = observed_difference, distribution = permutation_stock_zero)
```
```{r contrasts}
# ## Psychology: Coin - Zero *
# permutation_coin_zero <- data %>%
#   filter(Faculty == 'psychology') %>%
#   filter(Condition != 'stock') %>%
#   permutaion_test(data = ., group_var = 'Condition')
# observed_difference <- data %>%
#   filter(Faculty == 'psychology') %>%
#   filter(Condition != 'stock') %>%
#   group_by(Condition) %>%
#   summarise(mean = mean(cmx),
#             sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_coin_zero)
# 
# ## Psychology: Coin - Stock
# permutation_coin_stock <- data %>%
#   filter(Faculty == 'psychology') %>%
#   filter(Condition != 'zero') %>%
#   permutaion_test(data = ., group_var = 'Condition')
# observed_difference <- data %>%
#   filter(Condition != 'zero') %>%
#   group_by(Condition) %>%
#   summarise(mean = mean(cmx),
#             sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_coin_stock)
# 
# ## Psychology: Stock - zero
# permutation_stock_zero <- data %>%
#   filter(Faculty == 'psychology') %>%
#   filter(Condition != 'coin') %>%
#   permutaion_test(data = ., group_var = 'Condition')
# observed_difference <- data %>%
#   filter(Condition != 'coin') %>%
#   group_by(Condition) %>%
#   summarise(mean = mean(cmx),
#             sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_stock_zero)
# 
# ## Chemistry: Coin - Zero
# permutation_coin_zero <- data %>%
#   filter(Faculty == 'chemistry') %>%
#   filter(Condition != 'stock') %>%
#   permutaion_test(data = ., group_var = 'Condition')
# observed_difference <- data %>%
#   filter(Faculty == 'chemistry') %>%
#   filter(Condition != 'stock') %>%
#   group_by(Condition) %>%
#   summarise(mean = mean(cmx),
#             sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_coin_zero)
# 
# ## Chemistry: Coin - Stock
# permutation_coin_stock <- data %>%
#   filter(Faculty == 'chemistry') %>%
#   filter(Condition != 'zero') %>%
#   permutaion_test(data = ., group_var = 'Condition')
# observed_difference <- data %>%
#   filter(Faculty == 'chemistry') %>%
#   filter(Condition != 'zero') %>%
#   group_by(Condition) %>%
#   summarise(mean = mean(cmx),
#             sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_coin_stock)
# 
# ## Chemistry: Stock - zero *
# permutation_stock_zero <- data %>%
#   filter(Faculty == 'chemistry') %>%
#   filter(Condition != 'coin') %>%
#   permutaion_test(data = ., group_var = 'Condition')
# observed_difference <- data %>%
#   filter(Faculty == 'chemistry') %>%
#   filter(Condition != 'coin') %>%
#   group_by(Condition) %>%
#   summarise(mean = mean(cmx),
#             sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_stock_zero)
```

```{r contrast_faculty}
# ## Chemistry - Psychology: coin
# permutation_faculty <- data %>%
#   filter(Condition == 'coin') %>%
#   permutaion_test(data = ., group_var = 'Faculty')
# observed_difference <- data %>%
#   filter(Condition == 'coin') %>%
#   group_by(Faculty) %>%
#   summarise(mean = mean(cmx),
#               sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_faculty)
# 
# ## Chemistry - Psychology: stock
# permutation_faculty <- data %>%
#   filter(Condition == 'stock') %>%
#   permutaion_test(data = ., group_var = 'Faculty')
# observed_difference <- data %>%
#   filter(Condition == 'stock') %>%
#   group_by(Faculty) %>%
#   summarise(mean = mean(cmx),
#               sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_faculty)
# 
# ## Chemistry - Psychology: zero
# permutation_faculty <- data %>%
#   filter(Condition == 'zero') %>%
#   permutaion_test(data = ., group_var = 'Faculty')
# observed_difference <- data %>%
#   filter(Condition == 'zero') %>%
#   group_by(Faculty) %>%
#   summarise(mean = mean(cmx),
#               sd = sd(cmx))
# p_value(observed_difference = observed_difference, distribution = permutation_faculty)

```

Before conducting a more detailed analysis, we tested the hypothesis whether the mathematical experience and the task definition condition affect the overall value of the series complexity. For testing these hypotheses we followed following one-sided permutation test of mean difference procedure. First, from the results, we resampled without replacement relevant groups 100,000 times. Second, for each resampling, we computed the difference between group means. Third, we calculated p-value as the proportion of the number of resampled difference greater than the observed difference to all resampled differences (the reproducible R script is in Appendix A). To test the hypothesis that students with more mathematical experience will produce series more complex than others we compared the difference between mean algorithmic complexity of series produced by chemistry students and psychology students. The difference between mean algorithmic complexity in group of psychology students ($M = .757,\ SD = .223$) and chemistry students ($M = .771, SD = .218$) was insignificant, $p = .33$. 

Although this result suggests that the mathematical experience does not affect the overall algorithmic complexity of produced series we would argue that this conclusion needs further examination. In this study, we interfered with mathematical experience based on the field of studies. Therefore, we did not control for individual differences. 

However, there were significant differences of mean algorithmic complexity between the task definiton coditions. The  mean algorithmic complexity in the Coin Tossing Condition ($M = .797,\ SD = .193$) was greater than in the No Instruction Condition ($M = .698,\ SD = .267$), $p = .01$. Simirarly, the mean algorithmic complexity in the Stock Market Condition ($M = .793, SD = .184$) was greater than in the No Instruction Condition ($M = .698,\ SD = .267$), $p = .01$. Only the difference in means between the Coin Tossing Condition and the Stock Martket Condition was insignificant, $p = .46$.



It indicates that the task definition conditions affect the overall algorithmic complexity of produced series. In the conditions (the Coin Tossing and the Stock Market conditions), in which task was defined in less abstract manner participants produced more complex series than in the No Instruction condition.

In a more detailed analysis, we used the rolling algorithmic complexity. We used the linear mixed-effects model to test following hypotheses: 1) the algorithmic complexity decreases over time, 2) the task definition moderates the effect of time on algorithmic complexity of the series, and, 3) the mathematical experience moderates the effect of time on algorithmic complexity of the series. We present herein analysis for a sliding window of the length 8 only. That is because this model fitted the data the best. The specification of the rest of the models is in Appendix A together with a replicable R script. The dependent variable (the algorithmic complexity) in the models was logged so back-transformed model predictions were bounded to be non-negative since there is no notion of negative randomness / algorithmic complexity. In all models as fixed effects we entered the timestep, the task abstraction condition (with No Instruction Condition as a reference level in dummy coding), the mathematical experience condition (with Chemistry as a reference level in dummy coding) and interaction terms between the timestep and the task abstraction condition, and between the timestep and the mathematical experience condition. Goodness-of-fit of the models were assessed with marginal $R^2$ (variance retained by fixed effects only) and conditional $R^2$ (variance retained by the model as such).

```{r plot1}
seq8 %>%
  ungroup() %>%
  mutate(predict = predict(model8.5),
         ymin = predict - 1.96*predict(model8.5, se.fit = TRUE, level = 0)$se.fit,
         ymax = predict + 1.96*predict(model8.5, se.fit = TRUE, level = 0)$se.fit) %>%
  group_by(idx) %>%
  summarise(predict = mean(predict),
            ymin = mean(ymin),
            ymax = mean(ymax),
            cmx = mean(log(cmx_w))) %>%
  ggplot(aes(x = (idx), y = predict)) +
  geom_smooth(aes(ymin = ymin, ymax=ymax), stat = 'identity', inherit.aes = TRUE, color = 'black', size = .5) +
  geom_line(aes(x = (idx), y = cmx), alpha = .2, color = 'black') +
  labs(title = "Figure 1. Change in the rolling algorithmic complexity over the Time step",
       x = "Time step",
       y = "Logarithm of the Algorithmic Complexity")
  
```

In the model with the sliding window of length 8 as random effects, we had an intercept for subjects. The fitted model indicated several significant effects. There was a significant negative effect of the timestep on the algorithmic complexity, $t(49376)=4.798,\ p<.001$. With each timestep, the algorithmic complexity decreased $.009\%\ \pm.002$. This result supported the first hypothesis regarding the observed decline in the algorithmic complexity over time. However, hypothesis two and three were not supported. Neither the task definition nor the level of mathematical experience moderated the effect of timestep on the algorithmic complexity (the detailed results are presented in Table 1.). However, similarly to the overall algorithmic complexity, there were significant differences between task definition conditions.  The algorithmic complexity in the Coin Tossing Condition was $2.814\%\ \pm.954$ higher than in the No Instruction Condition, $t(169)=2.948,\ p=.004$. Similarly, in the Stock Market condition, the algorithmic complexity was higher $2.82\%\ \pm.962$ than in the No Instruction Condition, $t=2.931,\ p=.004$. 

The results suggest that although there is a significant difference between task definition conditions in average algorithmic complexity it does not affect the effect of the timestep on the algorithmic complexity. The decline is constant across all conditions.

```{r table1}
Predictors_names <- rownames(coef(summary(model8.5)))
coef(summary(model8.5)) %>% 
  as.tibble() %>% 
  mutate(Predictors = Predictors_names) %>%
  select(Predictors,
         "Estimates" = 1,
         "Std. Error" = 2,
         "df*"=DF,
         "t-value",
         "p-value") %>%
  kable(caption = "Table 1. Model specification") %>%
  footnote(general = 'Marginal R2 = 3.93%, Conditional R2=37.48% \nStd. Deviation of the random individual effects s=.048, p<.001 \n\\*Deegres of Freedom were adjusted with the Satterthwaiter Method \nThe model accounted for first-order autoregressive errors corelation structure')
```

```{r plot2}
seq8 %>%
  mutate(Condition = case_when(Condition == "zero" ~ "No Instruction",
                               Condition == "stock" ~ "Stock Market",
                               Condition == "coin" ~ "Coin Tossing")) %>%
  group_by(Condition, idx) %>%
  summarise(cmx_w = mean(cmx_w)) %>%
  ggplot(aes(x = factor(Condition, levels = c("Coin Tossing", "Stock Market", "No Instruction")), y = (cmx_w), fill = Condition)) +
  geom_boxplot(show.legend = FALSE) +
  stat_boxplot(geom = "errorbar", width = .2) +
  labs(title = "Figure 2. The difference in the algorithmic complexity between task definition conditions",
       x = "Condition",
       y = "Algorithmic complexity")

```

### Study 2.2.

In the second study, participants were recruited through the Polish Nationwide Opinion Poll Ariadna. It is a Polish Online Opinion Poll often used to conduct political surveys or scientific research. Depending on the declared length of the study users are gratified with points which they can exchange for prizes. During the research, participants were asked to produce 10 twelve elements binary series and to complete Polish versions of the Need for Cognition and the Need for Clouser Scales. 

```{r load_and_prepare_data2}
data2 <- read_sav("dane/Zbiór_Biesaga_los_ostateczny_v1.sav") %>%
  mutate(id = 1:n()) %>%
  select(-Id) %>%
  filter(survey_finish_time >= 656 & survey_finish_time < 1708) %>%
  rename_at(vars(matches("^v1_r\\d+")),
            ~str_extract(string = .x, pattern = '\\d+$') %>%
              str_c("pzp", .)) %>%
  rename_at(vars(matches("^v2_r\\d+")),
            ~str_extract(string = .x, pattern = '\\d+$') %>%
              str_c('pp', .)) %>%
  mutate_at(vars(matches("pp\\d+")), ~as.numeric(.)) %>%
  mutate_at(vars(matches("pzp\\d+")), ~as.numeric(.))
```

```{r cfa}
# nfc_model <- 'preferowanie_porzadku =~ pzp1 + pzp6 + pzp17 + pzp19 + pzp26 + pzp27 + pzp28
#          preferowanie_przewidywalnosci =~ pzp5 + pzp7 + pzp9 + pzp15 + pzp16 + pzp21 + pzp22 + pzp32
#          nietolerowanie_wieloznacznosci =~ pzp3 + pzp8 + pzp12 + pzp24 + pzp25 + pzp29
#          zamknietosc_umyslowa =~ pzp2 + pzp4 + pzp20 + pzp23 + pzp30 + pzp31
#          zdecydowanie =~pzp10 + pzp11 + pzp13 + pzp14 + pzp18
# 
#          preferowanie_porzadku ~~ 1*preferowanie_porzadku
#          preferowanie_przewidywalnosci ~~ 1*preferowanie_przewidywalnosci
#          nietolerowanie_wieloznacznosci ~~ 1*nietolerowanie_wieloznacznosci
#          zamknietosc_umyslowa ~~ 1*zamknietosc_umyslowa
#          zdecydowanie ~~ 1*zdecydowanie
# 
#          pzp1 ~~ pzp1
#          pzp2 ~~ pzp2
#          pzp3 ~~ pzp3
#          pzp4 ~~ pzp4
#          pzp5 ~~ pzp5
#          pzp6 ~~ pzp6
#          pzp7 ~~ pzp7
#          pzp8 ~~ pzp8
#          pzp9 ~~ pzp9
#          pzp10 ~~ pzp10
#          pzp11 ~~ pzp11
#          pzp12 ~~ pzp12
#          pzp13 ~~ pzp13
#          pzp14 ~~ pzp14
#          pzp15 ~~ pzp15
#          pzp16 ~~ pzp16
#          pzp17 ~~ pzp17
#          pzp18 ~~ pzp18
#          pzp19 ~~ pzp19
#          pzp20 ~~ pzp20
#          pzp21 ~~ pzp21
#          pzp22 ~~ pzp22
#          pzp23 ~~ pzp23
#          pzp24 ~~ pzp24
#          pzp25 ~~ pzp25
#          pzp26 ~~ pzp26
#          pzp27 ~~ pzp27
#          pzp28 ~~ pzp28
#          pzp29 ~~ pzp29
#          pzp30 ~~ pzp30
#          pzp31 ~~ pzp31
#          pzp32 ~~ pzp32
#          
#          pzp1 ~ 1
#          pzp2 ~ 1
#          pzp3 ~ 1
#          pzp4 ~ 1
#          pzp5 ~ 1
#          pzp6 ~ 1
#          pzp7 ~ 1
#          pzp8 ~ 1
#          pzp9 ~ 1
#          pzp10 ~ 1
#          pzp11 ~ 1
#          pzp12 ~ 1
#          pzp13 ~ 1
#          pzp14 ~ 1
#          pzp15 ~ 1
#          pzp16 ~ 1
#          pzp17 ~ 1
#          pzp18 ~ 1
#          pzp19 ~ 1
#          pzp20 ~ 1
#          pzp21 ~ 1
#          pzp22 ~ 1
#          pzp23 ~ 1
#          pzp24 ~ 1
#          pzp25 ~ 1
#          pzp26 ~ 1
#          pzp27 ~ 1
#          pzp28 ~ 1
#          pzp29 ~ 1
#          pzp30 ~ 1
#          pzp31 ~ 1
#          pzp32 ~ 1
#          '
# nfc_fit <- cfa(nfc_model,
#                std.lv = TRUE,
#                missing = 'fiml',
#                data = data2)
# summary(nfc_fit, fit.measures=TRUE, standardized = TRUE)
# semPaths(nfc_fit, what="std", 
#          sizeLat = 7, sizeMan = 7, edge.label.cex = .75)
# parameterEstimates(nfc_fit, standardized = TRUE)
# resid(nfc_fit, type = "cor")$cov
# modificationIndices(nfc_fit, sort.=TRUE, minimum.value=3)
```

```{r tests_calculations}
data2 <- data2 %>%
  mutate_at(vars(matches("pp[2, 3, 8, 10, 13, 15, 17, 19, 22, 26, 28, 33, 34, 35]")), ~{6-.}) %>%
  mutate_at(vars(matches("pzp[2, 5, 10, 14, 15, 16, 17, 18, 20, 24, 30, 31]")), ~{7-.}) %>%
  mutate(year = as.numeric(year)) %>%
  select(-age) %>%
  rowwise() %>%
  mutate(preferowanie_porzadku = sum(pzp1, pzp6, pzp17, pzp19, pzp26, pzp27, pzp28),
         preferowanie_przewidywalnosci = sum(pzp5, pzp7, pzp9, pzp15, pzp16, pzp21, pzp22, pzp32),
         nietolerowanie_wieloznacznosci = sum(pzp3, pzp8, pzp12, pzp24, pzp25, pzp29),
         zamknietosc_umyslowa = sum(pzp2, pzp4, pzp20, pzp23, pzp30, pzp31),
         zdecydowanie = sum(pzp10, pzp11, pzp13, pzp14, pzp18),
         preferowanie_porzadku2 = sum(pzp6, pzp26, pzp27),
         preferowanie_przewidywalnosci2 = sum(pzp9, pzp21, pzp32),
         nietolerowanie_wieloznacznosci2 = sum(pzp3, pzp8, pzp29),
         zamknietosc_umyslowa2 = sum(pzp2, pzp20, pzp31),
         zdecydowanie2 = sum(pzp13, pzp14, pzp18),
         potrzeba_poznania = sum(pp1, pp2, pp3, pp4, pp5, pp6, pp7, pp8, pp9, pp10,
                                 pp11, pp12, pp13, pp14, pp15, pp16, pp17, pp18, pp19, pp20,
                                 pp21, pp22, pp23, pp24, pp25, pp26, pp27, pp28, pp29, pp30,
                                 pp31, pp32, pp33, pp34, pp35, pp36),
         warunek = as.character(warunek),
         warunek = if_else(warunek == "1", "homogeneous", "heterogeneous")) %>%
  select(-matches('pp\\d+|pzp\\d+'))

data2_time <- data2 %>%
  gather(key = "Index", value = "Bit", matches("war[[:digit:]]")) %>%
  filter(!is.na(Bit)) %>% 
  filter(Bit != 99) %>% 
  mutate(klucz = if_else(grepl(x = Index, pattern = "time"), 'time', 'seq'),
         ids = str_extract(Index, pattern = '^war1_\\d+|^war2g\\d+_|^war2rm\\d+_'),
         ids = if_else(grepl(x = ids, pattern = 'war1'),
                       str_extract(string = ids, pattern = '(\\d+)(?!.*\\d)'),
                       str_extract(string = ids, pattern = '([rmg]+\\d+)(?!.*\\d)')),
         ids = case_when(ids == 'rm1' ~ '1',
                         ids == 'g1' ~ '2',
                         ids == 'rm2' ~ '3',
                         ids == 'g2' ~ '4',
                         ids == 'rm3' ~ '5',
                         ids == 'g3' ~ '6',
                         ids == 'rm4' ~ '7',
                         ids == 'g4' ~ '8',
                         ids == 'rm5' ~ '9',
                         ids == 'g5' ~ '10',
                         TRUE ~ ids),
         ids = as.numeric(ids)) %>%
  select(id, ids, Bit, klucz) %>%
  group_by(id, ids, klucz) %>%
  mutate(idx = 1:n()) %>%
  spread(klucz, Bit) %>%
  filter(!is.na(seq))

data2_long <- data2_time %>%
  select(id, ids, seq) %>%
  group_by(id, ids) %>%
  summarise(seq = list(seq)) %>%
  filter(lengths(seq)>8)
```

#### 2.2.1. Procedure and Design

The study followed an experimental design, including one two-level between-subjects variable. First, participants were at random assigned to either the homogeneous instruction condition or the heterogeneous instruction condition. In both conditions, similarly to study 1, participants saw a red square every two seconds in ten 12 displays series. In the homogeneous condition for each series, they were asked to imagine tossing a fair coin and report the result whenever they saw a red square. In the heterogeneous instruction condition, their task altered every second series. In the odd series, they were asked to imagine tossing a fair coin and report the result whenever they saw a red square. In the even series, participants were asked to imagine a stock market chart and to assume that price fluctuation is generated by a random process. They were instructed to try to predict whether the prices will go up or down and to report the outcome every time they saw a red square. After completing the procedure of generating random series participants in both conditions were asked to fill in Polish versions of the Need for Cognition and the Need for Closure Scales.

#### 2.1.2. Participants

The participants were recruited through the Polish Nationwide Opinion Poll Ariadna. It is a Polish Online Opinion Poll often used to conduct political surveys or scientific research. Depending on the declared length of the study users are gratified with points which they can exchange for prizes. A total number of $266$ subjects agreed to take part in the study. However, due to the unrealistic (short or long) time of completion and unfinished surveys we excluded 80 participants. Therefore, finally we had a sample of 186 participants ($134$ females), aged from $18$ to $77$ ($M = 39.32,\ SD = 13.08$). They were at random assigned to one of the experimental conditions. The procedure was approved by the ethics committee of the Robert Zajonc Institute for Social Studies at the University of Warsaw. All participants gave informed consent before taking part in the study. 

#### 2.1.3. Data manipulation

In online research, it is crucial to measure survey time and exclude participants who completed the task in unrealistic (short or long) time. In our case, we removed 15\% of the shortest answers and 15\% of the longest. Although the participants were instructed to only press relevant keys when they saw a red square some people pressed it less frequent than 120 times. Therefore, the length of the series varies between subjects from 100 to 120 elements ($Median=114$). 

We used „pybdm” library in Python to compute algorithmic complexity of each series. It is an implementation of Block Decomposition Method which allows extending the power of Coding Theorem Method on longer strings. All other analyses were performed using R (R Core Team, 2019). We used packages "dplyr” (version 0.8.1), "magrittr” (version 1.5),  "lavaan" (version 0.6), and "ggplot2" (version 3.2.0) for data manipulation, processing, visualisation, and structural equation modeling. 

For each participant, we computed the overall value of the series complexity and the rolling algorithmic complexity. The latter was based on the computation of algorithmic complexity for a sliding window of length from 5 to 9. That is because the length of the working memory capacity is $7\pm2$ bits of information. We calculated algorithmic complexity for all the lengths separately and performed the analysis for all the lengths.

```{python prepare_strings2}
import numpy as np
import pandas as pd
from bdm import BDMRecursive as BDM

def window_bdm(seq, bdm, k=7):
    return np.array([ bdm.bdm(seq[i:(i+k)]) for i in range(len(seq) - k) ])
    
data = r.data2_long
data.id = data.id.astype(int)
data.ids = data.ids.astype(int)
data.seq = data.seq.apply(lambda x: np.array(x, dtype=int))

bdm = BDM(ndim=1, min_length=7)
seq7 = pd.DataFrame({'id': r.data2_long.id.astype(int),
                     'ids': r.data2_long.ids.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm))})

bdm = BDM(ndim=1, min_length=5)
seq5 = pd.DataFrame({'id': r.data2_long.id.astype(int),
                     'ids': r.data2_long.ids.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 5))})

bdm = BDM(ndim=1, min_length=9)
seq9 = pd.DataFrame({'id': r.data2_long.id.astype(int),
                     'ids': r.data2_long.ids.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 9))})

bdm = BDM(ndim=1, min_length=6)
seq6 = pd.DataFrame({'id': r.data2_long.id.astype(int),
                     'ids': r.data2_long.ids.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 6))})

bdm = BDM(ndim=1, min_length=8)
seq8 = pd.DataFrame({'id': r.data2_long.id.astype(int),
                     'ids': r.data2_long.ids.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm, k = 8))})
                     

                     


# seq5 = seq5.loc[seq5.cmx >= seq5.cmx.quantile(.05), :]
# seq7 = seq7.loc[seq7.cmx >= seq7.cmx.quantile(.05), :]
# seq9 = seq9.loc[seq9.cmx >= seq9.cmx.quantile(.05), :]
# seq6 = seq11.loc[seq6.cmx >= seq6.cmx.quantile(.05), :]
# seq8 = seq13.loc[seq8.cmx >= seq8.cmx.quantile(.05), :]
# seq15 = seq15.loc[seq15.cmx >= seq15.cmx.quantile(.05), :]
```


```{r prepare_data_for_models}
seq5 <- py$seq5
seq6 <- py$seq6
seq7 <- py$seq7
seq8 <- py$seq8
seq9 <- py$seq9

data2 <- data2_long %>%
  select(id, ids) %>%
  left_join(data2) %>%
  left_join(seq7 %>% select(id, ids,cmx)) %>%
  select(-matches('war\\d.')) %>%
  left_join(data2_time %>% select(id, ids, idx, time) %>% group_by(id,ids) %>% summarise(time = sum(time)))
  
seq5 <- seq5 %>%
    unnest() %>%
    group_by(id,ids) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data2, -cmx)) %>%
    left_join(data2_time %>%
               select(id, idx, ids, idx_time = time) %>%
               mutate(time_roll = rollsum(idx_time, k =5, align = "left", fill = NA)) %>%
               filter(!is.na(time_roll)) %>%
                select(id, idx, ids, time_roll))

seq6 <- seq6 %>%
    unnest() %>%
    group_by(id,ids) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data2, -cmx)) %>%
  left_join(data2_time %>%
               select(id, idx, ids, idx_time = time) %>%
               mutate(time_roll = rollsum(idx_time, k =6, align = "left", fill = NA)) %>%
               filter(!is.na(time_roll)) %>%
               select(id, idx, ids, time_roll))


seq7 <- seq7 %>%
    unnest() %>%
    group_by(id,ids) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data2, -cmx)) %>%
  left_join(data2_time %>%
               select(id, idx, ids, idx_time = time) %>%
               mutate(time_roll = rollsum(idx_time, k = 7, align = "left", fill = NA)) %>%
               filter(!is.na(time_roll)) %>%
               select(id, idx, ids, time_roll))

seq8 <- seq8 %>%
    unnest() %>%
    group_by(id,ids) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data2, -cmx)) %>%
  left_join(data2_time %>%
               select(id, idx, ids, idx_time = time) %>%
               mutate(time_roll = rollsum(idx_time, k =8, align = "left", fill = NA)) %>%
               filter(!is.na(time_roll)) %>%
               select(id, idx, ids, time_roll))

seq9 <- seq9 %>%
    unnest() %>%
    group_by(id,ids) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data2, -cmx)) %>%
  left_join(data2_time %>%
               select(id, idx, ids, idx_time = time) %>%
               mutate(time_roll = rollsum(idx_time, k =9, align = "left", fill = NA)) %>%
               filter(!is.na(time_roll)) %>%
               select(id, idx, ids, time_roll))

```
#### 2.2.4 Results
First, we computed algorithmic complexity for each series produced by participants. Afterward, we used the linear mixed-effects model to test following hypothesis: 1) the algorithmic complexity decreases over time, 2) the instruction condition moderates the effect of time on the algorithmic complexity, 3) the need for cognition increases the algorithmic complexity, 4) the order subscale of need for closure decreases the algorithmic complexity, 5) the predictability subscale of the need for cognition scale decrease the algorithmic complexity, 6) ambiguity subscale of the need for closure scale increases the algorithmic complexity, 7) closed-mindness subscale of the need for closure scale decreases the algorithmic complexity. The dependent variable (the algorithmic complexity) in the model was normlized  because even though people were asked to produce series of 12 elements, the length of produced series varied. Normalization of the algorithcmi complexity allowed to compare series of different lenghts. As fixed effects we entered the instruction condition (with the homogeneous condition as the reference level in dummy coding), the series number (we enetered also a quadratic term for the series number because it increased the model fit), the interaction term between series number and the instruction condition, the order subscale from the need for closure, the ambiguity subscale from the need for closure scale, the closed-mindness subscale from the need for closure scale, the predictibility subscale from the need for closure scale, and the need for cogniiotn scale. Goodness-of-fit of the model was assessed with marginal $R^2$ (variance retained by fixed effects only) and conditional $R^2$ (variance retained by the model as such). As random effects, we had an intercept for subjects and random effect of slope for the series number.

The fitted model indicated several significant effects. There was a significant negative effect of the series number on the algorithmic complexity, $t(1609)=4.396,\ p<.001$. With each series, the algorithmic complexity decreased by $.045\ \pm.01$ points. However, since the quadratic term was also present, $t(1609)=3.826,\ p<.001$, the linear decile of the algorithmic complexity was reduced by $.003\ \pm.0009$ point with each series. This result supported the first hypothesis regarding the observed decline in the algorithmic complexity over time (compare Figure 3.). However, the hypothesis 3 did not account for the decrease of the algorithmic complexity decline with each series. It seems that the algorithmic complexity at some point even increased this change in the trend requires further examination. However, hypothesis two was not supported. The instruction manipulation did not affect neither the change of the algorithmic complexity over series nor the algorithmic complexity itlesf. 

Additionaly, the fitted model allowed testing hypothesis regarding the Need for Cognition and subscales of the Need for Closure relationships with the algorithmic complexity. The Need for Cognition was possitevly correlated with the algorithmic complexity of the series, $t(175)=2.543, p < .05$. With the one point increase on the Need for Cognition scale there was $.003\ \pm.001$ increase in the algorithmic complexity. This result supported the hypotehsis that people who are more inclined to challenging tasks produces more complex series. Hypothesis 4 regarding the relationship between order subscale from the Need for Closure Scale and the algorithmic complexity was not supported. However, the probability subscale was negatively correlated to the algorithmic complexity, $t(175) = 2.402,\ p<.05$. With the one polint increase on the probability scale the algorithmic complexity declined by $-.012\ \pm.008$. This result supported hypothesis five regarding the relationship between probability subscale and the algorithmic complexity. Hypothesis 6 regarding the relationship between ambiguity subscale and the algorithmic complexity was not supported. However, the close-mindness subscale was negatively correlated with the algorithmic complexity, $t(175)=2.39,\ p<.05$. With the one point increase on the close-mindness subscale the algorithmic complexity declined by $-.017\ \pm.007$. The detailed results are presented in Table 2. (the reproducable R code is in Appendix A).


```{r model_all}
model <- lme(cmx ~ 1 + (ids)*warunek +
               preferowanie_porzadku2 +
               preferowanie_przewidywalnosci2 +
               nietolerowanie_wieloznacznosci2 +
               zamknietosc_umyslowa2 +
               potrzeba_poznania +
               survey_finish_time,
             random = ~ids|id,
             data = data2,
             correlation = corCAR1(form = ~ids|id), method = "ML")

model2 <- update(model, random = ~ids-1|id )
anova(model,model2)
model3 <- update(model, random = ~1|id)
anova(model,model3)

## Error Diagnostics
plot(model)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model2, abline = c(0,1))
plot(model, resid(., type = 'normalized') ~ fitted(.), type = c("p", "smooth"))
plot(model,resid(.,type="pearson") ~ (ids),type=c("p","smooth"))
plot(model,resid(.,type="pearson") ~ survey_finish_time,type=c("p","smooth"))
plot(model,resid(.,type="pearson") ~ preferowanie_porzadku2,type=c("p","smooth"))
plot(model,resid(.,type="pearson") ~ preferowanie_przewidywalnosci2,type=c("p","smooth"))
plot(model,resid(.,type="pearson") ~ nietolerowanie_wieloznacznosci2,type=c("p","smooth"))
plot(model,resid(.,type="pearson") ~ zamknietosc_umyslowa2,type=c("p","smooth"))
plot(model,resid(.,type="pearson") ~ potrzeba_poznania,type=c("p","smooth"))
#plot(model,resid(.,type="pearson") ~ year,type=c("p","smooth"))
boxplot(resid(model, type = 'normalized') ~ data2$warunek)

model_quadratic <- lme(cmx ~ 1 +ids*warunek + I(ids^2)*warunek +
               preferowanie_porzadku2 +
               preferowanie_przewidywalnosci2 +
               nietolerowanie_wieloznacznosci2 +
               zamknietosc_umyslowa2 +
               potrzeba_poznania,
             random = ~ids|id,
             data = data2,
             correlation = corCAR1(form = ~ids|id),
             method = "ML")

anova(model_quadratic, model)

model2 <- update(model_quadratic, random = ~ids-1|id )
anova(model_quadratic,model2)
model3 <- update(model_quadratic, random = ~1|id)
anova(model_quadratic,model3)

## Error Diagnostics
plot(model_quadratic)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model_quadratic, abline = c(0,1))
plot(model_quadratic, resid(., type = 'normalized') ~ fitted(.), type = c("p", "smooth"))
plot(model_quadratic,resid(.,type="pearson") ~ (ids),type=c("p","smooth"))
plot(model_quadratic,resid(.,type="pearson") ~ survey_finish_time,type=c("p","smooth"))
plot(model_quadratic,resid(.,type="pearson") ~ preferowanie_porzadku2,type=c("p","smooth"))
plot(model_quadratic,resid(.,type="pearson") ~ preferowanie_przewidywalnosci2,type=c("p","smooth"))
plot(model_quadratic,resid(.,type="pearson") ~ nietolerowanie_wieloznacznosci2,type=c("p","smooth"))
plot(model_quadratic,resid(.,type="pearson") ~ zamknietosc_umyslowa2,type=c("p","smooth"))
plot(model_quadratic,resid(.,type="pearson") ~ potrzeba_poznania,type=c("p","smooth"))
#plot(model,resid(.,type="pearson") ~ year,type=c("p","smooth"))
boxplot(resid(model_quadratic, type = 'normalized') ~ data2$warunek)

## Autocorrealtion Diagnostics
acf(resid(model_quadratic, type = 'normalized'))
pacf(resid(model_quadratic, type = 'normalized'))

## Summary
r.squaredGLMM(model_quadratic)
summary(model_quadratic)
```

```{r table2}
Predictors_names <- rownames(coef(summary(model_quadratic)))
coef(summary(model_quadratic)) %>% 
  as.tibble() %>% 
  mutate(Predictors = Predictors_names) %>%
  select(Predictors,
         "Estimates" = 1,
         "Std. Error" = 2,
         "df*"=DF,
         "t-value",
         "p-value") %>%
  kable(caption = "Table 2. Model specification") %>%
  footnote(general = 'Marginal R2 = 6.68%, Conditional R2=45.21% \nStd. Deviation of the random individual effects s=.015, p<.001 \n\\*Deegres of Freedom were adjusted with the Satterthwaiter Method \nThe model accounted for first-order autoregressive errors corelation structure')
```

```{r figure_3}
data2 %>%
  ungroup() %>%
  mutate(predict = predict(model_quadratic),
         ymin = predict - 1.96*predict(model_quadratic, se.fit = TRUE, level = 0)$se.fit,
         ymax = predict + 1.96*predict(model_quadratic, se.fit = TRUE, level = 0)$se.fit) %>%
  group_by(ids) %>%
  summarise(predict = mean(predict),
            cmx = mean(cmx),
            ymin = mean(ymin),
            ymax = mean(ymax)) %>%
  mutate(ids = (ids)) %>%
  ggplot(aes(x = (ids), y = predict)) +
  geom_smooth(aes(ymin = ymin, ymax=ymax), stat = 'identity', inherit.aes = TRUE, color = 'black', size = .5) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10)) +
  geom_jitter(data = data2 %>% ungroup, aes(x = (ids), y = cmx), alpha = .2, color = 'grey') +
  labs(title = "Figure 3. Change in the algorithmic complexity over the series number",
       x = "Series number",
       y = "Algorithmic Complexity")
```

 
```{r model52}
## Model selection
model5 <- lme(log(cmx_w) ~ 1 + idx*ids*warunek +
               log(time_roll),
             random = ~1|idx + idx|ids + ids|id,
             weights = varIdent(~warunek),
             data = seq5,
             correlation = corCAR1(form = ~1|id))
model5.5 <- update(model5,random = ~1|id)
anova(model5, model5.5)
model5.6 <- update(model5.5, random = ~idx-1|id )
anova(model5.6,model5)

## Error Diagnostics
plot(model5.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model5.5, abline = c(0,1))
plot(model5.5, resid(., type = 'normalized') ~ fitted(.), type = c("p", "smooth"))
plot(model5.5,resid(.,type="pearson") ~ (ids),type=c("p","smooth"))
plot(model5.5,resid(.,type="pearson") ~ log(time_roll),type=c("p","smooth"))
boxplot(resid(model5.5, type = 'normalized') ~ seq5$warunek)

## Autocorrealtion Diagnostics
acf(resid(model5, type = 'normalized'))
pacf(resid(model5, type = 'normalized'))

## Summary
r.squaredGLMM(model5)
summary(model5)
``` 
```{r model62}
## Model selection
model6 <- lme(log(cmx_w) ~ 1 + idx*(ids)*warunek +
               log(time_roll),
             random = ~1|idx+idx|ids+ids|id,
             weights = varIdent(~warunek),
             data = seq6,
             correlation = corCAR1(form = ~1|id))

model6.5 <- update(model6,random = ~1|id)
anova(model6, model6.5)
model6.6 <- update(model6, random = ~idx-1|id )
anova(model6.6,model6)
#model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
#model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
#anova(model3, model4, model5, model6, model7)

## Error Diagnostics
plot(model6.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model6.5, abline = c(0,1))
plot(model6.5, resid(., type = 'normalized') ~ fitted(.), type = c("p", "smooth"))
plot(model6.5,resid(.,type="pearson") ~ log(time_roll),type=c("p","smooth"))
plot(model6.5,resid(.,type="pearson") ~ (ids),type=c("p","smooth"))
plot(model6.5,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
#plot(model,resid(.,type="pearson") ~ year,type=c("p","smooth"))
boxplot(resid(model6.5, type = 'normalized') ~ seq6$warunek)

## Autocorrealtion Diagnostics
acf(resid(model6.5, type = 'normalized'))
pacf(resid(model6.5, type = 'normalized'))

## Summary
r.squaredGLMM(model6.5)
summary(model6.5)
``` 
```{r model72}
## Model selection
model7 <- lme(log(cmx_w) ~ 1 + idx*(ids)*warunek +
               log(time_roll),
             random = ~1|idx + idx|ids+ids|id,
             data = seq7,
             correlation = corCAR1(form = ~1|id))

model7.5 <- update(model7,random = ~1|id)
anova(model7, model7.5)
model7.6 <- update(model7.5, random = ~idx-1|id )
anova(model7.6,model7)
#model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
#model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
#anova(model3, model4, model5, model6, model7)

## Error Diagnostics
plot(model7.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model7.5, abline = c(0,1))
plot(model7.5, resid(., type = 'normalized') ~ fitted(.), type = c("p", "smooth"))
plot(model7.5,resid(.,type="pearson") ~ (ids),type=c("p","smooth"))
plot(model7.5,resid(.,type="pearson") ~ (idx),type=c("p","smooth"))
plot(model7.5,resid(.,type="pearson") ~ log(time_roll),type=c("p","smooth"))
boxplot(resid(model7.5, type = 'normalized') ~ seq7$warunek)

## Autocorrealtion Diagnostics
acf(resid(model7.5, type = 'normalized'))
pacf(resid(model7.5, type = 'normalized'))

## Summary
r.squaredGLMM(model7.5)
summary(model7.5)
``` 
 
```{r model82}
## Model selection
model8 <- lme(log(cmx_w) ~ 1 + idx*(ids)*warunek +
               log(time_roll),
             random = ~1|idx+idx|ids+ids|id,
             data = seq8,
             correlation = corCAR1(form = ~1|id))

model8.5 <- update(model8,random = ~1|id)
anova(model8, model8.5)
model8.6 <- update(model8.5, random = ~idx-1|id )
anova(model8.6,model8)
#model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
#model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
#anova(model3, model4, model5, model6, model7)

## Error Diagnostics
plot(model8.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model8.5, abline = c(0,1))
plot(model8.5, resid(., type = 'normalized') ~ fitted(.), type = c("p", "smooth"))
plot(model8.5,resid(.,type="pearson") ~ (ids),type=c("p","smooth"))
plot(model8.5,resid(.,type="pearson") ~ (idx),type=c("p","smooth"))
plot(model8.5,resid(.,type="pearson") ~ log(time_roll),type=c("p","smooth"))

#plot(model,resid(.,type="pearson") ~ year,type=c("p","smooth"))
boxplot(resid(model8.5, type = 'normalized') ~ seq8$warunek)

## Autocorrealtion Diagnostics
acf(resid(model8.5, type = 'normalized'))
pacf(resid(model8.5, type = 'normalized'))

## Summary
r.squaredGLMM(model8.5)
summary(model8.5)
``` 
```{r model92}
## Model selection
model9 <- lme(log(cmx_w) ~ 1 + idx*(ids)*warunek +
               log(time_roll),
             random = ~1|idx+idx|ids+ids|id,
             data = seq9,
             correlation = corCAR1(form = ~1|id))

model9.5 <- update(model9,random = ~1|id)
anova(model9, model9.5)
model9.6 <- update(model9.5, random = ~idx-1|id )
anova(model9.6,model9)
#model5 <- update(model2,correlation = corARMA(form = ~idx|id, p = 2))
#model6 <- update(model5, correlation = corARMA(form = ~idx|id, p = 7))
#anova(model3, model4, model5, model6, model7)

## Error Diagnostics
plot(model9.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model9.5, abline = c(0,1))
plot(model9.5, resid(., type = 'normalized') ~ fitted(.), type = c("p", "smooth"))
plot(model9.5,resid(.,type="pearson") ~ (ids),type=c("p","smooth"))
plot(model9.5,resid(.,type="pearson") ~ (idx),type=c("p","smooth"))
plot(model9.5,resid(.,type="pearson") ~ log(time_roll),type=c("p","smooth"))
#plot(model,resid(.,type="pearson") ~ year,type=c("p","smooth"))
boxplot(resid(model9.5, type = 'normalized') ~ seq9$warunek)

## Autocorrealtion Diagnostics
acf(resid(model9.5, type = 'normalized'))
pacf(resid(model9.5, type = 'normalized'))

## Summary
r.squaredGLMM(model9.5)
summary(model9.5)
``` 

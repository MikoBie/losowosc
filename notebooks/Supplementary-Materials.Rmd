---
title: "Supplementary Materials"
description:
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: no
bibliography: literature.bib
csl: apa-single-spaced.csl
---

## README

Herein, first, we present linear mixed models for Study 1 of different rolling
window lengths computed on all observations. The rolling algorithmic complexity
was calculated for window lengths from 5 to 9. This corresponds to the estimated
capacity of the working memory which is usually said to be 7 Â± 2 elements
[@baddeley1986]. Second, we perform the same analysis for Study 1 as in
`Main.Rmd` but on all observations (in the `Main.Rmd` we excluded observations
that exceeded 313 elements in random series generation task).

For each length of the rolling window first, we compared models with different
random effects. Second, we plotted diagnostic plots. Third, we computed the
conditional and marginal coefficient of determination to select the model which
explained the most variance.

*NOTE*: Although this is an `R` markdown we use package `reticulate` to import
some functions from `python`. Therefore, this script requires having an Anaconda
environment created. The detailed instruction on how to set up the virtual
environment and how to use it is described in the README.md file.

```{r setup_env, include=FALSE}
# Globals
ROOT <- here::here()
HERE <- file.path(ROOT, "notebooks")
```

```{r setup}
library(magrittr)
library(tidyverse)
library(quantreg) 
library(reticulate)
library(lattice)
library(zoo)
library(MuMIn)
library(acss)
library(haven)
library(nlme)
library(kableExtra)
library(ggpubr)
## Set ggplot theme
theme_set(theme_classic())
## Load conda environment
use_condaenv("bdm")
```

## Load and Prepare Data

```{r load_and_prepare_data}
## Load Study 1. raw data
data <- read.csv2(file.path(ROOT,"data/Study_1.csv")) %>% 
    rename_at(vars(matches("^X\\d")), ~str_replace_all(pattern = "X", replacement = 'd', .x)) %>%
    mutate_at(vars(matches("^d\\d")), as.integer) %>%
    rename(id = X)

## Transform to long format
data_long <- gather(data, key = "Index", value = "Bit", matches("^d\\d")) %>%
    filter(!is.na(Bit)) %>%
    arrange(id) %>%
    group_by(id) %>% 
    mutate(idx = 1:n()) %>%
    filter(idx < 313) %>%
    summarize(seq = list(Bit)) %>%
    ungroup 
``` 

## Compute algorithmic complexity

```{python prepare_strings}
## Import modules from conda environment
import numpy as np
import pandas as pd
from pybdm import BDM
from pybdm import PartitionIgnore, PartitionRecursive

## Create a function to compute algorithmic complexity
def window_bdm(seq, bdm, k=9):
    return np.array([ bdm.bdm(seq[i:(i+k)]) for i in range(len(seq) - k) ])

## Load r object to Python    
data = r.data_long
data.id = data.id.astype(int)
data.seq = data.seq.apply(lambda x: np.array(x, dtype=int))
bdm_ignore = BDM(ndim=1, partition=PartitionIgnore)

## Compute algorithmic complexity for the length 5                
bdm_recursive_5 = BDM(ndim=1, partition=PartitionRecursive, min_length=5)
seq5 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm_ignore.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm_recursive_5, k = 5))})
                   
## Compute algorithmic complexity for the length 6
bdm_recursive_6 = BDM(ndim=1, partition=PartitionRecursive, min_length=6)
seq6 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm_ignore.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm_recursive_6, k = 6))})
                     
## Compute algorithmic complexity for the length 7
bdm_recursive_7 = BDM(ndim=1, partition=PartitionRecursive, min_length=7)
seq7 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm_ignore.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm_recursive_7, k = 7))})

## Compute algorithmic complexity for the length 8                  
bdm_recursive_8 = BDM(ndim=1, partition=PartitionRecursive, min_length=8)
seq8 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm_ignore.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm_recursive_8, k = 8))})

## Compute algorithmic complexity for the length 9
bdm_recursive_9 = BDM(ndim=1, partition=PartitionRecursive, min_length=9)
seq9 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm_ignore.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm_recursive_9, k = 9))})
```

## Load data from Python to R

```{r data_processing}
## Read Python object to R
seq7 <- as_tibble(py$seq7)
seq5 <- as_tibble(py$seq5)
seq9 <- as_tibble(py$seq9)
seq6 <- as_tibble(py$seq6)
seq8 <- as_tibble(py$seq8)

## Join algorithmic complexity matrix with data
data <- select(data, -matches("^d\\d")) %>%
    filter(id %in% seq8$id) %>%
    left_join(select(seq8, id, cmx), by = "id") %>%
    mutate(Condition = case_when(Condition == "coin" ~ "Coin Tossing",
                                 Condition == "stock" ~ "Stock Market",
                                 Condition == "zero" ~ "No Instruction")) %>%
    mutate(Condition = factor(Condition, levels = c( "No Instruction", "Stock Market", "Coin Tossing")))

## Prepare data for mixed models
seq5 <- seq5 %>%
    unnest(cols = cmx_w) %>%
    mutate(cmx_w = as.vector(cmx_w)) %>%
    group_by(id) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    filter(idx < 313)

seq6 <- seq6 %>%
    unnest(cols = cmx_w) %>%
    mutate(cmx_w = as.vector(cmx_w)) %>%
    group_by(id) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    filter(idx < 313)

seq7 <- seq7 %>%
    unnest(cols = cmx_w) %>%
    mutate(cmx_w = as.vector(cmx_w)) %>%
    group_by(id) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    filter(idx < 313)

seq8 <- seq8 %>%
    unnest(cols = cmx_w) %>%
    mutate(cmx_w = as.vector(cmx_w)) %>%
    group_by(id) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    filter(idx < 313)

seq9 <- seq9 %>%
    unnest(cols = cmx_w) %>%
    mutate(cmx_w = as.vector(cmx_w)) %>%
    group_by(id) %>%
    mutate(idx = 1:n()) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    filter(idx < 313)
```

## Models with length 5

```{r model5}
## Model selection
model5 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
             random = ~idx|id,
             data = seq5)
model5.4 <- update(model5,correlation = corARMA(form = ~idx|id, p = 1))
anova(model5, model5.4)
model5.5 <- update(model5.4,random = ~1|id)
anova(model5.5, model5.4)
model5.6 <- update(model5.5, random = ~idx-1|id )
anova(model5.6,model5.4)

## Errors diagnostics
par(mfrow = c(1,2))
plot(model5.4)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model5.4, abline = c(0,1))
plot(model5.4, resid(., type = 'normalized') ~ fitted(.))
plot(model5.4,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
boxplot(resid(model5.4, type = 'normalized') ~ seq5$Condition)
boxplot(resid(model5.4, type = 'normalized') ~ seq5$Faculty)

## Autocorrelation Diagnostics
acf(resid(model5.4, type = 'normalized'))
pacf(resid(model5.4, type = 'normalized'))
acf(resid(model5.4))
pacf(resid(model5.4))

## Summary
r.squaredGLMM(model5.4)
summary(model5.4)

```

## Models with length 6

```{r model6}
## Model specification
model6 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
             random = ~idx|id,
             data = seq6)
model6.4 <- update(model6,correlation = corARMA(form = ~idx|id, p=1))
anova(model6, model6.4)
model6.5 <- update(model6.4,random = ~1|id)
anova(model6.5, model6.4)
model6.6 <- update(model6.5, random = ~idx-1|id )
anova(model6.6,model6.4)

## Error Diagnostics
plot(model6.4)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model6.4, abline = c(0,1))
plot(model6.4, resid(., type = 'normalized') ~ fitted(.), type = c('p', 'smooth'))
boxplot(resid(model6.4, type = 'pearson') ~ seq6$Condition)
boxplot(resid(model6.4, type = 'pearson') ~ seq6$Faculty)

## Autocorrelation Diagnostics
acf(resid(model6.4, type = 'normalized'))
pacf(resid(model6.4, type = 'normalized'))


## Summary
r.squaredGLMM(model6.4)
summary(model6.4)

```

## Models with length 7

```{r model7}
## Model Selection
model7 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
             random = ~idx|id,
             data = seq7)
model7.4 <- update(model7,correlation = corARMA(form = ~idx|id, p = 1))
anova(model7, model7.4)
model7.5 <- update(model7.4,random = ~1|id)
anova(model7.5, model7.4)
model7.6 <- update(model7.4, random = ~idx-1|id )
anova(model7.6,model7.4)

## Error Diagnostics
par(mfrow = c(1,2))
plot(model7.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model7.5, abline = c(0,1))
plot(model7.5, resid(., type = 'normalized') ~ fitted(.))
plot(model7.5,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
boxplot(resid(model7.5, type = 'normalized') ~ seq7$Condition)
boxplot(resid(model7.5, type = 'normalized') ~ seq7$Faculty)

## Autocorrelation Diagnostics
acf(resid(model7.5, type = 'normalized'))
pacf(resid(model7.5, type = 'normalized'))
acf(resid(model7.5))
pacf(resid(model7.5))

## Summary
r.squaredGLMM(model7.5)
summary(model7.5)

```

## Models with length 8

```{r model8}
## Model Selection
model8 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
             random = ~1|id,
             data = seq8)
model8.4 <- update(model8,correlation = corAR1(form = ~idx|id))
anova(model8, model8.4)
model8.5 <- update(model8.4,random = ~1|id)
anova(model8.5, model8.4)
model8.6 <- update(model8.5, random = ~idx-1|id )
anova(model8.6,model8.4)
model8.7 <- update(model8.5, correlation = corARMA(form = ~1|id, p = 1))

## Error Diagnostics
par(mfrow = c(1,2))
plot(model8.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model8.5, abline = c(0,1))
plot(model8.5, resid(., type = 'normalized') ~ fitted(.), type=c('p', 'smooth'))
plot(model8.5,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
boxplot(resid(model8.5, type = 'normalized') ~ seq8$Condition)
boxplot(resid(model8.5, type = 'normalized') ~ seq8$Faculty)

## Autocorrealtion Diagnostics
acf(resid(model8.5, type = 'normalized'))
pacf(resid(model8.5, type = 'normalized'))
acf(resid(model8.7))
pacf(resid(model8.7))

## Summary
r.squaredGLMM(model8.5)
summary(model8.5)

```

## Models with length 9

```{r model9}
## Model Selection
model9 <- lme(log(cmx_w) ~ 1 +idx*Faculty + idx*Condition,
             random = ~idx|id,
             data = seq9)
model9.4 <- update(model9,correlation = corARMA(form = ~idx|id, p = 1))
anova(model9, model9.4)
model9.5 <- update(model9.4,random = ~1|id)
anova(model9.5, model9.4)
model9.6 <- update(model9.5, random = ~idx-1|id )
anova(model9.6,model9.4)

## Error Diagonostics
par(mfrow = c(1,2))
plot(model9.5)
#qqnorm(model2,~ranef(., level = 1))
qqnorm(model9.5, abline = c(0,1))
plot(model9.5, resid(., type = 'normalized') ~ fitted(.))
plot(model9.5,resid(.,type="pearson") ~ idx,type=c("p","smooth"))
boxplot(resid(model9.5, type = 'normalized') ~ seq9$Condition)
boxplot(resid(model9.5, type = 'normalized') ~ seq9$Faculty)

## Autocorrelation Diagnostics
acf(resid(model9.5, type = 'normalized'))
pacf(resid(model9.5, type = 'normalized'))
acf(resid(model9.5))
pacf(resid(model9.5))

## Summary
r.squaredGLMM(model9.6)
summary(model9.5)

```

## Overall 

In this part, we present the same analysis as in `Main.Rmd` but without
exclusion of observations that exceeded 314 elements.

```{r wilcox-faculty}
## Compute Wicoxon Rank Test between Chemistry and Psychology students
c(
    list(Faculty = data),
    split(data, data$Condition)
)%>%
    map2(names(.), ~ {
        mutate(tidy(wilcox.test(cmx ~ Faculty, data = .x, exact = FALSE)), condition = .y, .before = 1L)
    }) %>%
    bind_rows %>%
    select(
        Condition = condition,
        W = statistic,
        p = p.value
    )
```

Before testing our main hypotheses, we investigated whether there were
differences between the two groups of participants: students from the Chemistry
and Psychology Faculties. We wanted to explore whether their knowledge and
differences in the curriculum may have affected the outcomes. We used a
non-parametric test to assess whether the distributions of algorithmic
complexity were systematically different between groups of observations. The
Wilcoxon Rank Sum Test revealed that the main difference was not significant, $W
= 4191.5,\ p=.9866$. The further analysis within experimental conditions also
did not yield significant differences in distributions of algorithmic complexity
between faculties (NI, $W = 451.5,\ p = .8083$; SMI, $W = 531,\ .3447$; CTI,
$W = 422,\ p = .3184$). Therefore, there was no reason to further explore these
differences in later analyses.

```{r kurskal-condition}
## Perform Kruskal-Wallis for experimental conditions
kruskal.test(cmx~Condition, data = data)
```
```{r quantreg_condition}
# Full quantile process model
proc <- rq(cmx~Condition, tau = -1, data = data, method = "br")

# Quartile model
qreg <- rq(cmx~Condition, tau = 1:3/4, data = data, method = "br")
summary(qreg, se = "rank")
```


To test the hypothesis regarding the effect of the task description
(H1) on the overall algorithmic complexity of human-generated
series, we used also a non-parametric test to assess whether the distributions
of algorithmic complexity were systematically different between groups of
observations. The Kruskal-Wallis test revealed a significant difference between
distributions of algorithmic complexity in task description conditions,
$\chi^2(2) = 7.0051, p = .0301$. The closer visual examination of the histograms
revealed that the main difference between distributions was in their left tails
(see the left panel of Figure 1). In the NI condition, the
frequency of the lowest results is relatively higher than in the CTI and SMI
conditions. Therefore, to better understand this effect, we used quantile
regression [@koenker_quantile_2001] to estimate the full quantile process
in the three experimental groups as well as calculate $95\%$ confidence
intervals for the first, second, and third quartiles. As
Figure 1 shows, there is a clear gap between low quantiles
(left tail) in the No Instruction condition compared to other conditions.
Furthermore, confidence intervals showed that there are significant differences
in terms of 1st quartiles between the NI condition and both SMI, $95\%\ CI\
[.087\ .458]$, and CTI conditions, $95\%\ CI\ [.085\ .408]$. Similarly, in the
case of the second quartile, significant differences were only observed between
NI and CTI $95\%\ CI\ [.048\ .087]$. We did not observe significant differences
in the case of the third quartile. These results confirm that the gap visible in
Figure 1 is statistically significant. This indicates that both the Stock Market
and Coin Tossing conditions prevented participants from producing series of low
algorithmic complexity. The differences in the task description conditions seem
not to affect the upper parts of the distributions (roughly above median). Thus,
the results suggest that the task description does not enhance the ability to
generate random-like sequences, as many researchers believed [@ayton1991], but
rather helps to activate it whatsoever, resulting in significantly less
trivially non-random sequences being produced.

```{r plot2, include = TRUE, fig.cap = "Left panel: Distributions of normalized algorithmic complexity in the experimental groups. Right panel: The distributions of the normalized algorithmic complexity of series across quantiles. In the first quartile, the distribution of normalized algorithmic complexity of series in NI condition was different than in SMI ($95%\ CI\ [.087\ .458]$) and CTI conditions ($95%\ CI\ [.085\ .408]$). Similarly, in the case of the second quartile, significant differences were only observed between NI and CTI $95%\ CI\ [.048\ .087]$. There were no significant differences between the distributions in the groups in the case of the third quartile."}
## Create an object for the plot of the histogram of normalized algorithmic complexity for each experimental condition
plt1 <- data %>%
    ggplot(aes(x = cmx, fill = Condition)) +
    geom_histogram(bins = 10, color = "white") +
    facet_wrap(~Condition, ncol = 1L) +
    labs(x = "Normalized Algorithmic Complexity", y = "Count") +
    guides(fill = FALSE)

## Create an object for the plot of distribution of normalized algorithmic complexity for each experimental condition
plt2 <- t(proc$sol) %>%
    as_tibble %>%
    select(
        tau,
        zero  = `(Intercept)`,
        coin  = `ConditionCoin Tossing`,
        stock = `ConditionStock Market`
    ) %>%
    mutate(
        "Coin Tossing" = zero + coin,
        "Stock Market" = zero + stock,
        "No Instruction" = zero
    ) %>%
    pivot_longer(-c(tau:stock), names_to = "Condition", values_to = "Qbar") %>%
    mutate(Condition = factor(Condition, levels = c("No Instruction", "Stock Market", "Coin Tossing"))) %>%
    ggplot(aes(x = tau, y = Qbar, color = Condition, group = Condition)) +
    geom_line(show.legend = FALSE) +
    geom_vline(xintercept = .5, linetype = 2L) +
    labs(x = "Quantile", y = "Normalized Algorithmic Complexity", color = "Conditions")

## Arrange both plots as one figure
figure2 <- ggarrange(plt1,plt2)

## Plot the figure
figure2
```

## References
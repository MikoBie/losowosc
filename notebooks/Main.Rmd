---
title: "The effect of context and individual differences in human-generated randomness"
description: |
  Method section.
output:
  bookdown::html_document2:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: no
    fig_caption: true
bibliography: literature.bib
csl: apa-single-spaced.csl
---
## Method

Our goal was to test the above-mentioned hypotheses in two studies. In Study 1
we tested hypotheses regarding the effect of task description (H1) on the
algorithmic complexity of human-generated series. Additionally, in all
experimental conditions, we investigated the dynamics of the randomness of
human-generated series to test the potential effect of mental fatigue (H2). In
Study 2, we further examined the dynamics of randomness in human-generated
series (H2) and the relationship between algorithmic complexity of
human-generated series with the Need for Cognition (H3).

```{r setup-env, include=FALSE}
# Globals
ROOT <- here::here()
HERE <- file.path(ROOT, "notebooks")
DATA <- file.path(ROOT, "data", "proc")
FIG <- file.path(ROOT, "png")

knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 5)
```

```{r setup}
## Load necessary packages
library(tidyverse)
library(broom)
library(lattice)
library(MuMIn)
library(lme4)
library(lmerTest)
library(kableExtra)
library(ggpubr)
library(quantreg) 
library(emmeans)
library(mgcv)
library(itsadug)

## Set ggplot theme
theme_set(theme_classic())
COLORS <- RColorBrewer::brewer.pal(8, "Set1")
options(
    ggplot2.discrete.color = COLORS,
    ggplot2.discrete.fill  = COLORS
)
```

## Study 1.

In the first study, participants were tested individually in sessions that
lasted about 15 minutes. They were simply asked to produce a binary series of
300 elements. We gathered the responses from the participants using a custom
software^[It is available on GitHub under MIT License
https://github.com/MikoBie/Survey.] that was written in Processing
[@reas2006processing]. The procedure was run on MacBook Pro 13,3-inch Early
2015 with OS X 10.11. It collected signals with $16\ ms$ delay. The stimuli were
displayed on a built-in monitor with $2560 \times 1600$ pixel resolution and
60Hz refresh rate.

```{r load-study1-data}
## Load Study 1 data for general analysis
data <- read.csv2(file.path(DATA, "Study1.csv")) %>%
   mutate(Condition = factor(Condition, levels = c( "No Instruction", "Stock Market", "Coin Tossing")))

## Load Study 1 data form in-depth analysis
seq8 <- read.csv2(file.path(DATA, "Study1_seq8.csv")) %>%
  mutate(Condition = fct_relevel(as.factor(Condition), "No Instruction"))
``` 

### Procedure and Design

The experiment followed a factorial design with one between-subjects variable ---
the task description. Participants were two groups of students recruited either
from the Psychology or Chemistry faculties at the University of Warsaw.
Participants from both groups were assigned randomly to one of the three
experimental conditions: 1) No Instruction (NI); 2) Coin Tossing Instruction
(CTI); and 3) Stock Market Instruction (SMI). In all three conditions, we used
custom software to run the experiment. Every 1.25 second it displayed a red
square for .75 second (cf. Scheme 1}).  Furthermore, in
the NI condition, participants were simply asked to select randomly
and press one of the two predefined keys every time they saw the red square. In
the CTI condition, participants were instructed to imagine a toss of a
fair coin whenever they saw the red square and press either key marked as tails
or heads. In the SMI condition, participants were asked to imagine a
stock market chart and to assume that price fluctuations were random. They were
instructed to try to predict whether the price in the next time step will go up
or down and to press either key marked ''up'' or ''down''.

![Scheme 1. Flow chart of Study 1 design. Both groups of participants were assigned at random to one of the three experimental conditions: 1) No Instruction	(NI); 2) Coin Tossing Instruction (CSI); and 3) Stock Market Instruction (SMI). In all three conditions, a custom software displayed every 1.25 second a red square for .75 second.](../png/figure_2.png)

### Participants

The participants were students from the Faculty of Psychology and
Faculty of Chemistry at the University of Warsaw.
A total of 183 subjects (129 females),
aged from 18 to 30 (\(M = 21.54,\ SD = 2.12\)), were randomly assigned to one
of the experimental conditions. The procedure was approved by the ethics
committee of Robert Zajonc Institute for Social Studies at the University
of Warsaw. All participants gave informed consent before taking part in the
study.

### Data processing

Although the participants were instructed to only press relevant keys when they
saw the red square some pressed it more and some less than 300 times. Therefore,
the length of the series varied between subjects from 218 to 1016 elements
(\(Median = 300\)). However, there were only 13 people who produced
significantly longer series than others. Therefore, in further analysis, we
removed observations exceeding the typical length of the series, i.e. the last
10\% of observations (those with more than 313 elements). This strategy did not
affect the results significantly because people who did not follow the task
scrupulously were spread almost uniformly between experimental conditions. After
the data preprocessing we had 56 participants in the NI condition;
54 in the SMI condition, and 55 in the CTI condition. The analysis of the
whole data set can be found in the Supplementary Materials.

We used ''pybdm''^[It is a Python package implementing Coding
Theorem and Block Decomposition methods for estimating algorithmic complexity
[@soler-toscano2014,@zenil_decomposition_2018]. It is available as a
standard package through PyPI https://pypi.org/project/pybdm/.] library
to estimate algorithmic complexity of series. All other analyses were
performed using R language [@rcore] with ''nlme'' and
''lme4'' packages for estimating linear mixed models
[@bates_fitting_2015, @nlme] and ''quantreg'' package for fitting
quantile regression [@koenker_quantreg_2020].

For each participant, we computed an overall algorithmic complexity of the
entire sequence as well as vectors of complexity estimates in rolling windows of
different lengths. Both were normalized (using the method described in
[@zenil_decomposition_2018]) because, even though we removed observations
with too long series (longer than 313 elements), the lengths of the remaining
sequences still varied. Normalized algorithmic complexity ranges from $0$ to $1$
where $0$ stands for the simplest possible object (a constant series filled with
a single symbol) and $1$ for the most complex object of a given size. The
rolling algorithmic complexity was calculated for window lengths from 5 to 9.
This corresponds to the estimated capacity of the working memory which is
usually said to be \(7\ \pm 2\) elements [@baddeley1986]. Herein, we
present the results only for the window of length 8. The rest can be found in
Supplementary Materials.

### Results

```{r wilcox-faculty}
## Compute Wicoxon Rank Test between Chemistry and Psychology students
c(
    list(Faculty = data),
    split(data, data$Condition)
)%>%
    map2(names(.), ~ {
        mutate(tidy(wilcox.test(cmx ~ Faculty, data = .x, exact = FALSE)), condition = .y, .before = 1L)
    }) %>%
    bind_rows %>%
    select(
        Condition = condition,
        W = statistic,
        p = p.value
    )
```

Before testing our main hypotheses, we investigated whether there were
differences between the two groups of participants: students from the Chemistry
and Psychology Faculties. We wanted to explore whether their knowledge and
differences in the curriculum may have affected the outcomes. We used a
non-parametric test to assess whether the distributions of algorithmic
complexity were systematically different between groups of observations. The
Wilcoxon Rank Sum Test revealed that the main difference was not significant, $W
= 3422.5,\ p=.9506$. The further analysis within experimental conditions also
did not yield significant differences in distributions of algorithmic complexity
between faculties (NI, $W = 431.5.5,\ p = .5172$; SMI, $W = 395,\ .5975$; CTI,
$W = 316,\ p = .3005$). Therefore, there was no reason to further explore these
differences in later analyses.

```{r kurskal-condition}
## Perform Kruskal-Wallis for experimental conditions
kruskal.test(cmx~Condition, data = data)
```
```{r quantreg_condition}
# Full quantile process model
proc <- rq(cmx~Condition, tau = -1, data = data, method = "br")

# Quartile model
qreg <- rq(cmx~Condition, tau = 1:3/4, data = data, method = "br")
summary(qreg, se = "rank")
```


To test the hypothesis regarding the effect of the task description
(H1) on the overall algorithmic complexity of human-generated
series, we used also a non-parametric test to assess whether the distributions
of algorithmic complexity were systematically different between groups of
observations. The Kruskal-Wallis test revealed a significant difference between
distributions of algorithmic complexity in task description conditions,
$\chi^2(2) = 7.9828, p = .0192$. The closer visual examination of the histograms
revealed that the main difference between distributions was in their left tails
(see the left panel of Figure 1). In the NI condition, the
frequency of the lowest results is relatively higher than in the CTI and SMI
conditions. Therefore, to better understand this effect, we used quantile
regression [@koenker_quantile_2001] to estimate the full quantile process
in the three experimental groups as well as calculate $95\%$ confidence
intervals for the first, second, and third quartiles. As
Figure 1 shows, there is a clear gap between low quantiles
(left tail) in the No Instruction condition compared to other conditions.
Furthermore, confidence intervals showed that there are significant differences
in terms of 1st quartiles between the NI condition and both SMI, $95\%\ CI\
[.097\ .468]$, and CTI conditions, $95\%\ CI\ [.086\ .463]$. Similarly, in the
case of the second quartile, significant differences were observed between NI
and SMI $95\%\ CI\ [.005\ .071]$, and NI and CTI $95\%\ CI\ [.009\ .087]$. We
did not observe significant differences in the case of the third quartile. These
results confirm that the gap visible in Figure 1 is
statistically significant. This indicates that both the Stock Market and Coin
Tossing conditions prevented participants from producing series of low
algorithmic complexity. The differences in the task description conditions seem
not to affect the upper parts of the distributions (roughly above median). Thus,
the results suggest that the task description does not enhance the ability to
generate random-like sequences, as many researchers believed
[@ayton1991], but rather helps to activate it whatsoever, resulting
in significantly less trivially non-random sequences being produced.

```{r plot2, include = TRUE, fig.cap = "Left panel: Distributions of normalized algorithmic complexity in the experimental groups. Right panel: The distributions of the normalized algorithmic complexity of series across quantiles. In the first quartile, the distribution of normalized algorithmic complexity of series in NI condition was different than in SMI ($95%\ CI\ [.0975\ .4677]$) and CTI conditions ($95%\ CI\ [.086\ .46311]$). Similarly, in the case of the second quartile, significant differences were observed between NI and SMI $95%\ CI\ [.0047\ .071]$, and NI and CTI $95%\ CI\ [.0091\ .0875]$. There were no significant differences between the distributions in the groups in the case of the third quartile."}
## Create an object for the plot of the histogram of normalized algorithmic complexity for each experimental condition
plt1 <- data %>%
    ggplot(aes(x = cmx, fill = Condition)) +
    geom_histogram(bins = 10, color = "white") +
    facet_wrap(~Condition, ncol = 1L) +
    labs(x = "Normalized Algorithmic Complexity", y = "Count") +
    guides(fill = FALSE)

## Create an object for the plot of distribution of normalized algorithmic complexity for each experimental condition
plt2 <- t(proc$sol) %>%
    as_tibble %>%
    select(
        tau,
        zero  = `(Intercept)`,
        coin  = `ConditionCoin Tossing`,
        stock = `ConditionStock Market`
    ) %>%
    mutate(
        "Coin Tossing" = zero + coin,
        "Stock Market" = zero + stock,
        "No Instruction" = zero
    ) %>%
    pivot_longer(-c(tau:stock), names_to = "Condition", values_to = "Qbar") %>%
    mutate(Condition = factor(Condition, levels = c("No Instruction", "Stock Market", "Coin Tossing"))) %>%
    ggplot(aes(x = tau, y = Qbar, color = Condition, group = Condition)) +
    geom_line(show.legend = FALSE) +
    geom_vline(xintercept = .5, linetype = 2L) +
    labs(x = "Quantile", y = "Normalized Algorithmic Complexity", color = "Conditions")

## Arrange both plots as one figure
figure2 <- ggarrange(plt1,plt2)

## Plot the figure
figure2

## Write out the figure to the file
figure2 %>% ggsave(filename = file.path(FIG,"figure_3.png"),
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 10)
```

```{r model-seq8}
## Define null linear mixed model for the rolling window of length 8
lmm0 <- lmer(cmx_w ~ idx + Faculty * Condition + (1 | id), data = seq8)

## Define linear mixed model for the rolling window of length 8 with subject-specific random intercepts and slopes (for task description conditions)
lmm1 <- lmer(
    formula = cmx_w ~ Condition + idx + Faculty +
        (0 + dummy(Condition, "No Instruction") | id) +
        (0 + dummy(Condition, "Coin Tossing") | id) +
        (0 + dummy(Condition, "Stock Market") | id),
    data = seq8)

## Print out summary of the model
summary(lmm1)

## Compare results between experimental conditions
compare <- emmeans(lmm1, "Condition", lmerTest.limit = 53164) %>%
    pairs(rev = TRUE, adjust = "holm")

## Print out the results of comparison
compare

## Compute effect sizes
eff_size(compare, edf = 161, sigma = sigma(lmm1))

## Compute conditional and marginal coefficient of terminnation for mixed models
r.squaredGLMM(lmm1)

## Compute confidence intervals of standard deviation in all three experimental conditions
prof <- profile(lmm1, which = c("theta_"), signames = FALSE)
(ci <- confint(prof))

## Compare null model with model with subject-specific random intercepts and slopes (for task description conditions)
anova(lmm0, lmm1)
```

In a more detailed analysis, we used the rolling algorithmic complexity. We
estimated a linear mixed-effects model to test the second hypothesis that
algorithmic complexity decreases over time (i.e. the effect of mental fatigue).
We present herein the analysis for a rolling window of length 8 only as this model
attained the best goodness-of-fit. The specification of the rest of the models
is in Supplementary Materials together with a reproducible R script.
The dependent variable in the models was (normalized) algorithmic complexity.
All models included, as fixed effects, the following factors: time step and task
description type (with NI condition used as the reference level).
Goodness-of-fit of the models was assessed with marginal \(R^2\) (variance
retained by fixed effects only) and conditional \(R^2\) (variance retained by
a model as such) as proposed by [@nakagawa_coefficient_2017].

In the rolling window model random intercepts and slopes for task description
conditions were included (clustered by subjects). The model indicated several
significant effects.
There was a significant negative effect of the time step on
the algorithmic complexity, \(t(47529.5568) = -18.62,\ p < .001\).
With each time step,
the algorithmic complexity decreased by \(.0002\ \pm .00001\).
This result supports the second hypothesis regarding the observed decline
in the algorithmic complexity over time. Additionally, there were significant
differences between task description conditions
(see Table 1). In SMI condition algorithmic
complexity was higher, \(.1156\ \pm.0338\), than in NI condition, \(t(74.99) =
3.418,\ d = .516,\ p = .001\). Similarly, the algorithmic complexity in CTI
condition was \(.1062\ \pm.0391\) higher than in NI Condition, \(t(102.68) =
2.714,\ d = .479,\ p = .008\). Although most of the fixed effects were
statistically significant they explained only $4.01\%$ of the variation while
after including the random effects the fraction was $45.21\%$. The closer
examination revealed that the model with subject-specific random intercepts and
slopes (for task description conditions) fitted the data better than the model
with random intercepts only, $\chi^2(1)= 33.075, p < .001$. Therefore, the
between-subjects variance was the function of the task description conditions.
The profiled confidence intervals of standard deviation in all three
experimental conditions (cf. Figure 2) show that in NI
condition between-subjects variance was significantly higher than in CTI (an
analysis of deviance of the model which included independent terms for the
variance in all three task description conditions against the model which
included an independent term for variance CTI condition was significant,
$\chi^2(1) = 35.537,\ p < .001$) and SMI conditions (an analysis of deviance of
the model which included independent terms for the variance in all three task
description conditions against the model which included an independent term for
variance in the Stock Market condition was significant $\chi^2(1) = 4.1442,\ p =
.0418$). These outcomes are in line with the quantile regression results for the
overall algorithmic complexity. The reason why people perform better in CTI and
SMI conditions than in NI condition is the fact that in the former they can
usually avoid trivial series with near-zero (normalized) complexity. Hence, on
average, the difference between average complexity produced by two different
people in these conditions is lower than in NI condition.

```{r variance-test}
## Model which included independent terms for the variance together for No Instruction and Coin Tossing Conditions and separetly for Stock Market
lmm_zero_coin0 <- lmer(
    formula = (cmx_w)~ Condition + id +
        (0 + I(dummy(Condition, "No Instruction") + dummy(Condition, "Coin Tossing")) | id) +
        (0 + dummy(Condition, "Stock Market") | id),
    data = seq8, 
    REML = FALSE
)

## Model which included independent terms for the variance together for the No Instruction and Stock Market Conditions and separately for Stock Market
lmm_zero_stock0 <- lmer(
  formula = (cmx_w) ~ Condition + id +
    (0 + I(dummy(Condition, "No Instruction") + dummy(Condition, "Stock Market")) | id) +
    (0 + dummy(Condition, "Coin Tossing") | id),
  data = seq8,
  REML = FALSE)
                  
## Define linear mixed model for the rolling window of length 8 with subject-specific random intercepts and slopes (for task description conditions)
lmm_zero <- lmer(
    formula = (cmx_w) ~ Condition + id +
        (0 + dummy(Condition, "No Instruction") | id) +
        (0 + dummy(Condition, "Coin Tossing") | id) +
        (0 + dummy(Condition, "Stock Market") | id),
    data = seq8,
    REML = FALSE
)

## Perform deviance analysis (compare models)
(aov_zero_coin <- anova(lmm_zero_coin0, lmm_zero))
(aov_zero_stock <- anova(lmm_zero_stock0, lmm_zero))

```
```{r plot3, include = TRUE, fig.cap = "Estimated standard deviations (with $95%$ CI based on likelihood profiling) of the distributions of random intercepts in the rolling window model (with length 8) in the three experimental conditions. The standard deviation of the residual distribution serves as a reference point. In NI condition between-subjects variance of the algorithmic complexity was higher than in SMI ($p = .012$) and CTI conditions ($p = .001$)."}
## Create data set with random effects
eff <- tibble(
    effect = fct_inorder(c("No Instruction", "Stock Market", "Coin Tossing", "Residuals")),
    sd = as.data.frame(VarCorr(lmm1))$sdcor,
    lo = ci[, 1],
    hi = ci[, 2]
)

## Create an object for the plot of the distribution of random intercepts in the rolling window mode (with length 8)
figure3 <- eff %>%
    mutate(effect = factor(effect, levels = c( "No Instruction", "Stock Market", "Coin Tossing", "Residuals"))) %>%
    ggplot(aes(x = effect, y = sd, color = effect)) +
    geom_segment(aes(xend = effect, y = lo, yend = hi), alpha = .5, size = 2, show.legend = FALSE) +
    geom_point(shape = 21L, fill = "white", size = 3, show.legend = FALSE) +
    geom_signif(comparisons = list(c("No Instruction", "Stock Market"),
                                   c("No Instruction", "Coin Tossing")),
                color = "black",
                linetype = 2L,
                annotations = c("p = .0417", "p < .001"),
                step_increase = .1) +
    
    labs(x = "Conditions", y = "Standard deviation of the Normalized Algorithmic Complexity", color = "")

## Plot the figure
figure3

## Write out the figure to the file
figure3 %>%
  ggsave(filename = file.path(FIG,"figure_4.png"),
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 6)
```

```{r table1, include=TRUE}
## Write out the specification of the model in a table
Predictors_names <- rownames(coef(summary(lmm1)))
coef(summary(lmm1)) %>% 
  as.tibble() %>% 
  mutate(Predictors = Predictors_names) %>%
  select(Predictors,
         "Estimates" = 1,
         "Std. Error" = 2,
         "df*"=df,
         "t-value" = 4,
         "p-value" = 5) %>%
  kable(caption = "Estimated parameters of the rolling window model.") %>%
  footnote(general = 'Marginal R2 = 4.01%, Conditional R2=45.21% \\nStd. Deviation of the random individual effects s=.048, p<.001 \\n\\*Degrees of Freedom were adjusted with the Kenward-Roger Method')
```

```{r plot4, , include = TRUE, fig.cap = "Solid lines present trend curves for normalized algorithmic complexity and dashed lines depict the average algorithmic complexity as a function of experimental conditions. For each participant, we computed vectors of complexity estimates in a rolling window of length 8. Although the experimental task asked for the creation of 300-long series the length still varied. Therefore, the uncertainty of both the trend curve and the average algorithmic complexity increased around the 292nd element. Regardless of the condition, with each time step the algorithmic complexity decreased by $.0002\ +/- .00001$, $t(47529.5568) = -18.62, p < .01$. However, in SMI condition algorithmic complexity was higher, $.1156\ +/- .0338$, that in NI condition, $t(74.99) = 3.418, d = .516, p = .001$. Similarly, the algorithmic complexity in CTI condition was $.1062\ +/- .0391$ higher than in NI Condition, $t(102.68) = 2.714, d = .479, p = .008$."}

seq8$predict <- predict(lmm1)
figure4 <- seq8 %>% 
  group_by(idx, Condition) %>% 
  summarise(predict = mean(predict),
            cmx_w = mean(cmx_w)) %>%
  ungroup() %>%
  mutate(Condition = factor(Condition, levels = c('No Instruction', 'Stock Market', 'Coin Tossing'))) %>%
  ggplot(aes(x = idx, y = (predict), group = Condition, color = Condition)) +
  geom_line(linetype = 1) +
  geom_line(aes(x = idx, y = cmx_w, color = Condition), alpha = .2, linetype = 'dashed') +
  labs(x = "Time step (Series element)", y = "Normalized Algorithmic Complexity", color = "") + 
  theme(legend.position = 'bottom')


figure4 

## Write out the figure to the file
figure4 %>%
  ggsave(filename = file.path(FIG,"figure_5.png"),
       device = "png",
       dpi = 300,
       height = 5,
       width = 6)
```
Altogether, the results of Study 1 support H1 and
H2. The task description affected the randomness of the
human-generated series. Subjects produced more random sequences when the
instructions included examples of random processes (tossing a fair coin or
random fluctuations in a stock market chart) than when there was no instruction
and participants were simply asked to produce random series. The difference was
mainly because in these conditions, unlike the No Instruction condition,
subjects were usually able to avoid producing trivial series of low randomness
(e.g. composed of almost only of 1's or 0's), hence, the between-subjects
variance of algorithmic complexity was smaller in these groups. However, we
argue that our results, based on a detailed analysis of quantiles of the
distributions in all experimental groups, provide more insight into the effects
of task instruction in random sequence generation and show that important
effects occur not at the level of averages/central tendencies but rather in
lower tails of complexity distributions meaning that the contextual cues have
inherently nonlinear effects. Additionally, the results of Study 1 support the
second hypothesis that the randomness of the series decreases over time (compare
Figure 3).

## Study 2.

The second study aimed to further investigate the observed effect of mental
fatigue on the random series production (H2) and to examine the
relationship of algorithmic complexity with the need for engaging cognitive
resources in challenging tasks (H3). The results
of Study~1 showed that the algorithmic complexity of human-generated series
decreased over time.  We argue that this effect was caused by fatigue due to the
task requiring constant attention and putting more effort to maintain the high
performance over time.  Participants in an almost 15 minutes-long procedure were
asked to produce 300 elements series without any break. However, our main
interest in Study 1 was to investigate the effect of context. Therefore, we used
Linear Mixed Models (LMM) approach. It allowed for detailed examination of
differences in variance of algorithmic complexity between experimental
conditions and estimation of the trend curve at the same time. In Study 2, we
wanted to focus more on the shape of the trend curve because the LMM approach
did not account for the fact that normalized algorithmic complexity is bounded
within $[0, 1]$. We investigated whether the effect of fatigue would be also
present between series of shorter independent tasks since many researchers
argued that the reason behind people's inability to produce random-like series
is the fact that subjects do not perceive each element as independent
[@kareev1995]. Moreover, [@hockey2011] showed that the mental
fatigue-related decrease of the performance in cognitively demanding tasks might
be inhibited by novelty. That is because the new task does not require
additional motivation that maintains its attractiveness and consequently
requires putting less effort. Therefore, in this study, participants were asked
to produce 10 independent binary series and to complete the Polish version of
the Need for Cognition scale [@matusz2011,@cacioppo1982]. To facilitate the
notion of independence between produced series and to allow for novelty between
tasks, we introduced two experimental conditions. In the Homogeneous Instruction
condition, participants produced ten independent binary series. Before each
series, they were instructed using the Coin Tossing Instruction from Study 1. In
the Heterogeneous Instruction condition, the Coin Tossing Instruction and the
Stock Market Instruction from Study~1 were alternated every second series.
Therefore, in this condition, each participant produced five series under CTI
and five under SMI. We expected that participants assigned to the Heterogeneous
condition would be able to maintain a high level of algorithmic complexity
between series for longer periods of time than participants in the Homogeneous
condition.  Participants were recruited through a Polish nationwide opinion poll
Ariadna. It is an online poll often used to conduct political surveys or
scientific research. Depending on the declared length of a study users are
gratified with points which they can exchange for prizes.

```{r load-study2-data}
## Load Study 2 data
data2 <- read.csv2(file.path(DATA, "Study2.csv")) %>%
  mutate(Condition = as.factor(Condition),
         IsHomogenous = if_else(Condition == "homogeneous", 0, 1),
         id = as.factor(id))
```

### Procedure and Design


We used an experimental design including one two-level between-subjects
variable. First, participants were assigned at random to either a Homogeneous
instruction condition or a Heterogeneous instruction condition. In both
conditions, similarly to Study 1, they were presented with a red square every
1.25 seconds for .75 seconds in ten 12 displays-long series (compare Scheme 2).
Thus, each subject was supposed to generate 10 sequences of 12 elements. In the
homogeneous condition, in all series, participants followed the Coin Tossing
Condition from Study 1. In the heterogeneous condition, their task altered every
second series. In the odd series, they were asked to follow the Coin Tossing
Condition from Study 1. In the even series, their task was identical as in the
Stock Market Condition in Study 1. Regardless of the condition, there was 5
seconds break between each series during which a blank slide with information on
how many series were left was presented. After completing the procedure of
generating random series participants in both conditions filled in the Polish
version of the Need for Cognition scale [@matusz2011,cacioppo1982].

![Scheme 2. Flow chart of Study~2 design. Participants were assigned at random to either Homogeneous or Heterogeneous instruction conditions. In both conditions, they were presented with a red square every 1.25 second for .75 second in ten 12 displays-long series. In the Homogeneous condition, in all series, participants were asked to follow the same instruction as in the Study 1 Coin Tossing Condition.  In the Heterogeneous condition, their task altered every second series. In the odd series, they were asked to follow the instruction from the Coin Tossing Condition from Study 1. In the even series, participants followed the identical instruction as in the Stock Market Condition in Study 1. Regardless of the condition, there was 5 seconds break between each series during which they were presented with a blank slide with information on how many series remained.  Afterward, in both conditions, participants were asked to complete the Polish version of the Need for Cognition scale.](../png/figure_6.png)

### Participants

A total number of 266 subjects agreed to take part in the study. However, due
to the unrealistic (too short or too long) time of completion and unfinished
surveys we excluded 80 participants. Therefore, finally, a sample of 186
participants (134 females), aged from 18 to 77 (\(M = 39.32,\ SD = 13.08\)),
was used. They were assigned at random to one of the experimental conditions.
The procedure was approved by the ethics committee of Robert Zajonc
Institute for Social Studies at the University of Warsaw. All participants
gave informed consent before taking part in the study.

### Data processing

In online research, it is crucial to measure survey time and exclude
participants who complete tasks in a suspiciously short or long amount of time.
Therefore, we removed observations with an unrealistic length of completion time
of the whole study (both the random series generation task and the Need for
Cognition scale), i.e. \(15\%\) of observations with the shortest response times
(below $11$ minutes) and \(15\%\) with the longest response times (above $28$
minutes).  Although the participants were instructed to only press relevant keys
when they see a red square some people pressed it less frequently than 120 times
(ten sequences of 12 elements). Therefore, the total number of produced elements
in all series varied between subjects from 100 to 120 elements (\(Median =
114\)).

As in Study~1, we used ''pybdm'' library for Python to estimate the
algorithmic complexity of generated
series^[https://pypi.org/project/pybdm/].  The complexity of each
sequence was estimated independently, so each participant was represented by ten
data points ordered as a time series. The algorithmic complexity of each series
was normalized because although the instruction asked to produce 12 elements
binary series the lengths varied.

We measured the need for engaging cognitive resources in challenging tasks with
the Polish version of the Need for Cognition scale developed by
[@matusz2011]. It contained 36 statements. Participants were asked to make a
judgment on a 5-point Likert scale on how much they agree with the statement
(with 1 --- I strongly disagree and 5 --- I strongly agree). The total score was
calculated as the sum of all answers. The psychometric quality of the Polish
version of the Need for Cognition (internal consistency, temporal stability,
factor analysis, criterion, and experimental validity) was confirmed in several
studies [@matusz2011]. Similarly, to the original scale by
[@cacioppo1982] the Polish adaptation has one major factor and an average
reliability coefficient $\alpha = .895$.

```{r model-all}
## Create dummy variable for contrasts
data2$Condition0 <- as.ordered(data2$Condition)
contrasts(data2$Condition0) <- "contr.treatment"

## Define a generalized additive mixed model
gamm1 <- gamm(cmx ~ need_for_cognition +
               Condition0 +
               s(ids, bs = 'tp', k = 10) +
               s(ids, by = Condition0, bs = 'tp', k = 10),
              random = list(id =~ 1, id =~ ids),
              data = data2)

## Print out summary of gam model
summary(gamm1$gam)

## Print out summary of lme model
summary(gamm1$lme)

## Compute conditional and marginal coefficient of terminnation for mixed models
r.squaredGLMM(gamm1$lme)
```

### Results

Unlike Study 1, in Study 2, we
used a generalized additive mixed model (GAMM) which can capture more complex
nonlinear trends than linear mixed models (LMM).} Additionally, this family of
models allowed for testing whether changing task descriptions between series
production (heterogeneous condition) would diminish the fatigue effect compared
to the production of independent series of binary series (homogeneous
condition). The dependent variable was normalized algorithmic complexity of a
currently generated series (10 data points per subject). The mean difference
between experimental conditions was represented with a single fixed parametric
effect with the homogeneous condition used as the reference group. The second
fixed parametric effect was a linear regression term corresponding to the
effect of the Need for Cognition. For nonparametric effects, we had a
non-linear difference in trends of algorithmic complexity over time between the
homogenous and heterogeneous conditions (with the homogeneous condition being
the reference level in dummy coding) and a non-linear trend of algorithmic
complexity over time. Additionally, we used subject-level random intercepts and
slopes for the time trend in order to model systematic between-subjects
differences. The goodness-of-fit of the model was assessed with marginal
\(R^2\) (variance retained by fixed effects only) and conditional \(R^2\)
(variance retained by the model as such) as proposed by
\cite{nakagawa_coefficient_2017}.

The fitted model explained $50.7\%$ of the variance with fixed effects
reproducing $10.67\%$. The non-linear trend of algorithmic complexity over time
was significant (cf. Figure 4), $F(edf=3.587,
Ref.df=3.587) = 11.863, p < .0001$. The negative effect of time was
approximately linear for the first five series and then the curve flattened and
remained at approximately the same level for the rest of the series. However,
neither linear ($t(180) = -1.69, p = .091$) nor non-linear ($F(edf = 1, Ref.df =
1) = 1.956, p = .162$) differences between the Homogeneous and Heterogeneous
conditions were significant. The alternation of task description between series
did not inhibit the effect of fatigue. Additionally, there was a small
significant parametric effect of the Need for Cognition, ($t(180) = 2.66, p =
.009$). With an average of $.01$ point increase on the Need for Cognition scale,
there was $.2\ \pm.093$ point change of the algorithmic complexity of the
series. This positive relationship between the randomness of series and the Need
for Cognition scale provided support for H3. People with a
tendency to engage in intellectually challenging tasks performed better in
random-series generation tasks.

```{r plot5, include = TRUE, fig.cap = "The trend curve for normalized algorithmic complexity for subsequent generated series with 95% confidence interval. For each series produced by participants, we estimated its algorithmic complexity. Therefore, each participant was represented by ten data points ordered in time sequences. The algorithmic complexity of each series was normalized because, even though the instruction asked to produce 12-elements binary sequences, the lengths varied."}
## Create data set with estimated non-linear trend and standard errors
data_plt4 <- get_predictions(gamm1$gam,
                        cond = list(ids = seq(min(data2$ids),
                                              max(data2$ids),
                                              length = 10)),
                        se = TRUE,
                        print.summary = FALSE)

## Create an object for the plot of the trend curve
figure5 <- data_plt4 %>%
  ggplot(aes(x = ids, y = fit)) +
  geom_line(aes(color = Condition0), show.legend = FALSE)  +
  geom_ribbon(aes(ymin = fit-CI, ymax = fit + CI, group = Condition0), alpha = .1) +
  scale_x_continuous(breaks=seq(0,10,1))+
  ylab("Normalized Algorithmic Complexity") +
  xlab("Series Number")

## Plot the figure
figure5 

## Write out the figure to the file
figure5 %>%
  ggsave(filename = file.path(FIG,"figure_7.png"),
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 6)
```
## References

<!-- CSS styling -->
<style>
    p {
        text-align: justify;
        height: 100%;
    }
</style>
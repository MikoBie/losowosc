---
title: "XXX"
description: |
    Method section.
author:
  - name: Miko≈Çaj Biesaga and Szymon Talaga
    affiliation: The Robert Zajonc Institute for Social Studies
    affiliation_url: www.iss.uw.edu.pl/en/
output: 
    pdf_document: 
      keep_tex: yes
---

\section{Method}

Our goal was to investigate described above hypothesis in two studies. In study 1., in experimental design, we tested hypotheses regarding the effect of task description (H1) and mathematical experience (H2) on the algorithmic complexity of human-generated series. Additionally, we investigated the dynamic of the randomness of human-generated series over time (H3). In study 2, we further examined the dynamic of randomness in human-generated series (H3) and the relationship between the algorithmic complexity of human-generated series with the need for cognition (H4).

```{r setup_env, include=FALSE}
# Globals
ROOT <- here::here()
HERE <- file.path(ROOT, "notebooks")
DATA <- file.path(ROOT, "data", "proc")
FIG <- file.path(ROOT, "png")

knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.width = 8,
                      fig.asp = 1,
                      fig.path = file.path(ROOT, "png"))
```

```{r setup}
## Load necessary packages
library(tidyverse)
library(broom)
library(lattice)
library(MuMIn)
library(lme4)
library(lmerTest)
library(kableExtra)
library(ggpubr)
library(quantreg) 
library(emmeans)
library(mgcv)
library(itsadug)
## Set ggplot theme
theme_set(theme_classic())
COLORS <- RColorBrewer::brewer.pal(8, "Set1")
options(
    ggplot2.discrete.color = COLORS,
    ggplot2.discrete.fill  = COLORS
)
```

\subsection{Study 1}

In the first study, participants were tested individually in sessions that lasted about 15 minutes. They were simply asked to produce a 300 elements binary series. We gathered the responses from the participants using a specially designed computer tool\footnote{It is available on GitHub under MIT License \url{https://github.com/MikoBie/Survey}.}.

```{r load_study1_data}
data <- read.csv2(file.path(DATA, "Study1.csv")) %>%
   mutate(Condition = factor(Condition, levels = c( "No Instruction", "Stock Market", "Coin Tossing")))

seq8 <- read.csv2(file.path(DATA, "Study1_seq8.csv")) %>%
  mutate(Condition = fct_relevel(as.factor(Condition), "No Instruction"))
``` 

\subsubsection{Procedure and Design}

The experiment followed a 2 x 3 factorial design, including two between-subjects variables: the mathematical experience and the task description condition. We recruited as participants two groups of students who either studied Psychology or Chemistry at the University of Warsaw. We assumed that students who chose as their major Psychology had a relatively small experience with concepts like randomness while students from the Chemistry Department were more familiar with it. We based our assumption on the number of obligatory courses students had to take in both departments. For Chemistry it was nearly 200 hours of subjects like Math, Physics, and Statistics during the first three years, while for Psychology it was just 90 hours of Statistics for five years. In both groups, participants were at random assigned to one of three experimental conditions: 1) the No Instruction Condition, 2) the Coin Tossing Condition, and 3) the Stock Market Condition. In all three conditions, we used a specially designed computer tool. It displayed every two seconds a red square. In the No Instruction Condition participants were simply asked to press in random order one of the two specially marked keys whenever they see a red square. In the Coin Tossing Condition, participants were instructed to imagine tossing a fair coin whenever they see a red square and press either key marked as tail or head. In the Stock Market Condition, participants were asked to imagine a stock market chart and to assume that price fluctuation is generated by a random process. They were instructed to try to predict whether the price in the next time step will go up or down and to press either key marked as an arrow up or down.

\subsubsection{Participants}

The participants were students from the Psychology Department and the Chemistry Department at the University of Warsaw. A total of \(183\) subjects (\(129\ females\)), aged from 18 to 30 (\(M = 21.54,\ SD = 2.12\)), were randomly assigned to one of the experimental conditions. The procedure was approved by the ethics committee of the Robert Zajonc Institute for Social Studies at the University of Warsaw. All participants gave informed consent before taking part in the study.

\subsubsection{Data manipulation}

Although the participants were instructed to only press relevant keys when they saw a red square some people pressed it more often and others less frequently than 300 times. Therefore, the length of the series varied between subjects from 218 to 1016 elements (\(Median = 300\)). However, there were only a few people who produced significantly longer series than others. Therefore, we cut off observations exceeding the typical length of the series, i.e.~we cut off the last 10\% of observations (with indexes longer than 313).

We used ''pybdm'' library in Python to compute the algorithmic complexity of each series\footnote{It is available on GitHub under MIT License \url{https://github.com/sztal/pybdm}.}. It is an implementation of the Block Decomposition Method which allows extending the power of the Coding Theorem Method on longer strings [@zenil2018].

For each participant, we computed the overall value of the series algorithmic complexity and the rolling algorithmic complexity. The former was normalized because even though we cut off 10\% of the longest indexes, the length of series still varied. Normalization of the algorithmic complexity allowed us to compare series of different lengths. The rolling algorithmic complexity was based on the computation of algorithmic complexity for a sliding window of length from 5 to 9. That is because the length of the working memory capacity is \(7\ \pm 2\) chunks of information [@baddeley1986]. Herein, we present the analysis only for the sliding window of length 8, but other analyses with reproducible R code might be found in Supplementary Materials.

\subsubsection{Results}

Before conducting a more detailed analysis, we tested hypotheses regarding the effect of the task description (H1) and mathematical experience (H2) on the overall algorithmic complexity of human-generated series. For testing these hypotheses we used non-parametric tests to assess whether the distributions of algorithmic complexity were systematically different between groups of observations.

```{r kurskal_condition}
kruskal.test(cmx ~ Condition, data = data)
```
```{r quantreg_condition}
# Full quantile process model
proc <- rq(cmx ~ Condition, tau = -1, data = data, method = "br")

# Quartile model
qreg <- rq(cmx ~ Condition, tau = 1:3/4, data = data, method = "br")
summary(qreg, se = "rank")
```

```{r plot2, include = TRUE, fig.cap = "Left panel: Histograms of Normalized Algorithmic Complexity in all three conditions. Right panel: The distributions of the normalized Algorithmic Complexity of series across quantiles. In the first quantile ($\tau = .25$), the distribution of the normalized algorithmic complexity of series in the No Instruction Condition was different than in the Stock Market Condition ($95\%\ CI [0.0868\ 0.4576]$) and in the Coin Tossing Condition ($95\%\ CI [0.0851\ 0.4079]$). In the second quantile ($\tau = .5$), the distribution difference was only significant between the No Instruction Condition and the Coin Tossing Condition, $95\%\ CI [0.0118\ 0.872]$. There were no significant differences in distributions between task description conditions in the third and fourth quantiles."}
plt1 <- data %>%
    ggplot(aes(x = cmx, fill = Condition)) +
    geom_histogram(bins = 10, color = "white") +
    facet_wrap(~Condition, ncol = 1L) +
    labs(x = "Normalized Algorithmic Complexity", y = "Count") +
    guides(fill = FALSE)

plt2 <- t(proc$sol) %>%
    as_tibble %>%
    select(
        tau,
        zero  = `(Intercept)`,
        coin  = `ConditionCoin Tossing`,
        stock = `ConditionStock Market`
    ) %>%
    mutate(
        "Coin Tossing" = zero + coin,
        "Stock Market" = zero + stock,
        "No Instruction" = zero
    ) %>%
    pivot_longer(-c(tau:stock), names_to = "Condition", values_to = "Qbar") %>%
    mutate(Condition = factor(Condition, levels = c("No Instruction", "Stock Market", "Coin Tossing"))) %>%
    ggplot(aes(x = tau, y = Qbar, color = Condition, group = Condition)) +
    geom_line(show.legend = FALSE) +
    geom_vline(xintercept = .5, linetype = 2L) +
    labs(x = "Quantile", y = "Normalized Alogorithmic Complexity", color = "Conditions")

figure2 <- ggarrange(plt1,plt2)

figure2

figure2 %>% ggsave(filename = file.path(FIG,"figure_2.png"),
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 10)
```

The Kruskal-Wallis Rank Sum Test revealed a significant difference between distributions of algorithmic complexity in task description's experimental conditions, $\chi^2(2) = 7.0051, p < .0301$. The closer visual examination of the histograms revealed that the main variation between distributions is observed between their left tails (compare left panel of Figure 2.). In the No Instruction condition, the frequency of the lowest results is relatively higher than in the Coin Tossing and the Stock Market conditions. Therefore, to better understand this effect, we performed quantile regression across quantiles of the distributions. As the dependent variable we entered normalized algorithmic complexity and as the fixed effect the task description condition (with No Instruction Condition as a reference level in dummy coding). The analysis revealed that in the first quantile ($\tau = .25$) there was a significant difference in distributions between the Stock Market Condition and the No Instruction Condition, $95\%\ CI\ [0.0868\ 0.4576]$ and the Coin Tossing Condition and the No Instruction Condition, $95\%\ CI\ [0.0851\ 0.4079]$. In the second quantile ($\tau = .5$), there was a significant difference in distributions between the Coin Tossing Condition and the No Instruction Condition only, $95\%\ CI\ [0.0118\ 0.872]$. There were no significant differences in distributions in the third and fourth quantiles (compare right panel of Figure 2.). These results indicate that both the Stock Market and Coin Tossing conditions prevent people from producing series of low algorithmic complexity. The differences in the task description conditions seem not to affect the distributions in their higher ends, therefore they do not facilitate producing more complex series as many researchers believed \citep[compare]{ayton1991].


```{r wilcox_faculty}
c(
    list(Faculty = data),
    split(data, data$Condition)
)%>%
    map2(names(.), ~{
        mutate(tidy(wilcox.test(cmx ~ Faculty, data = .x, exact = FALSE)), condition = .y, .before = 1L)
    }) %>%
    bind_rows %>%
    select(
        Condition = condition,
        W = statistic,
        p = p.value
    )
```

To test the second hypothesis that students with more mathematical experience will produce series more complex than others we compared distributions of algorithmic complexity in groups of chemistry and psychology students. The Wilcoxon Rank Sum Test revealed that the main difference was not siginificant, $W = 4191.5,\ p=.9866$. The further analysis within experimental conditions also did not yield significant differences in distributions of algorithmic complexity between faculties (No Instruction Condition, $W = 451.5,\ p = .8083$; Stock Market Condition, $W = 531,\ .3447$; Coin Tossing Condition, $W = 422,\ p = .3184$). Although this result suggests that the mathematical experience does not affect the overall algorithmic complexity of produced series we would argue that this conclusion needs further examination. In this study, we interfered with mathematical experience based on the field of studies. Therefore, we did not control for the actual mathematical experience but based our assumption on the number of classes students have taken over the course of their academic career.

```{r model_seq8}
lmm0 <- lmer(log(cmx_w) ~  idx + Faculty * Condition + (1 | id), data = seq8)

lmm1 <- lmer(
    formula = log(cmx_w) ~ Condition + idx + Faculty +
        (0 + dummy(Condition, "No Instruction") | id) +
        (0 + dummy(Condition, "Coin Tossing") | id) +
        (0 + dummy(Condition, "Stock Market") | id),
    data = seq8)

summary(lmm1)

compare <- emmeans(lmm1, "Condition", lmerTest.limit = 53164) %>%
    pairs(rev = TRUE, adjust = "holm")

compare
eff_size(compare, edf = 179, sigma = sigma(lmm1))
r.squaredGLMM(lmm1)

prof <- profile(lmm1, which = c("theta_"), signames = FALSE)
(ci <- confint(prof))

anova(lmm0, lmm1)
```

In a more detailed analysis, we used the rolling algorithmic complexity. We performed the linear mixed-effects model to test the third hypothesis that the algorithmic complexity decreases over time. We present herein analysis for a sliding window of length 8 only. That is because this model fitted the data the best. The specification of the rest of the models is in Supplementary Materials together with a replicable R script. The dependent variable (the algorithmic complexity) in the models was logged so back-transformed model predictions were bounded to be non-negative since there is no notion of negative randomness / algorithmic complexity. In all models as fixed effects we had the time step, the task description condition (with No Instruction Condition as a reference level in dummy coding), and the mathematical experience condition (with Chemistry as a reference level in dummy coding). Goodness-of-fit of the models were assessed with marginal \(R^2\) (variance retained by fixed effects only) and conditional \(R^2\) (variance retained by the model as such).

In the model with the sliding window of length 8 as random effects, we had an intercept for subjects as well as by-subject random slopes for task description conditions. The fitted model indicated several significant effects. There was a significant negative effect of the time step on the algorithmic complexity, \(t(52983.32) = -21.98,\ p < .01\). With each time step, the algorithmic complexity decreased by \(.004\%\ \pm.0002\). This result supported the third hypothesis regarding the observed decline in the algorithmic complexity over time. Additionaly, there were significant differences between task description conditions but not between the levels of mathematical experience (compare Table 1.). The algorithmic complexity in the Coin Tossing Condition was \(2.05\%\ \pm.8\) higher than in the No Instruction Condition, \(t(106.4) = 2.68,\ d = .399,\ p = .02\). Similarly, in the Stock Market condition, the algorithmic complexity was higher \(1.69\%\ \pm.74\) than in the No Instruction Condition, \(t(99.1) = 2.84,\ d = .313,\ p = .04\). Although listed fixed effects were statistically significant they explained only $2.83\%$ of the variance while random effects retained $46.82\%$ of the variance. The closer examination revealed that the model with intercept for subjects as well as by-subject random slopes for task description conditions better fitted the data than the model with only random intercept for subjects, $\chi^2(1)= 10.5, p < .001$. Therefore, in-between people variance was the function of the task description conditions. The profiled confidence intervals of standard deviation in all three experimental conditions, ploted in Figure 4., show that in No Instruction condition in-between people variance was significantly higher than in the Coin Tossing condition (the analysis of deviance of the model which included independent terms for the varaince in all three task description conditions against the model which included independent term for variance in Coin Tossing condition was significant .$\chi^2(1) =  12.021,\ p < .001$) and Stock Market condition (the analysis of deviance of the model which included independent terms for the varaince in all three task description conditions against the model which included independent term for variance in the Stock Market condition was significant $\chi^2(1) =  6.251,\ p = .0124$).  These results are in line with quantile regression results perfromed on the overall algorithmic complexity. The reason why people perform better in the Coin Tossing and Stock Market conditions than in the No Instruction Condition lies in the fact that in the former they produce series of smaller variability. The difference between the least and the most random series in these conditions is smaller than in the No Instruction Condition.


```{r variance_test}
lmm_zero_coin0 <- lmer(
    formula = log(cmx_w) ~ Condition + id +
        (0 + I(dummy(Condition, "No Instruction") + dummy(Condition, "Coin Tossing")) | id) +
        (0 + dummy(Condition, "Stock Market") | id),
    data = seq8, 
    REML = FALSE
)

lmm_zero_stock0 <- lmer(
  formula = log(cmx_w) ~ Condition + id +
    (0 + I(dummy(Condition, "No Instruction") + dummy(Condition, "Stock Market")) | id) +
    (0 + dummy(Condition, "Coin Tossing") | id),
  data = seq8,
  REML = FALSE)
                  

lmm_zero <- lmer(
    formula = log(cmx_w) ~ Condition + id +
        (0 + dummy(Condition, "No Instruction") | id) +
        (0 + dummy(Condition, "Coin Tossing") | id) +
        (0 + dummy(Condition, "Stock Market") | id),
    data = seq8,
    REML = FALSE
)

(aov_zero_coin <- anova(lmm_zero_coin0, lmm_zero))
(aov_zero_stock <- anova(lmm_zero_stock0, lmm_zero))

```
```{r plot3, include = TRUE, fig.cap = "The standard deviation of the Algorithmic Complexity in the sliding window of length 8 in all three experimental conditions with $95\%$ Confidence Intervals (standard deviation of residuals serves as a reference). In the No Instruction Condition in-between people variance of the Algorithmic Complexity was higher than in the Stock Market Condition ($p = .012$) and in the Coin Tossing Condition ($p = .001$)."}
eff <- tibble(
    effect = fct_inorder(c("No Instruction", "Stock Market", "Coin Tossing", "Residuals")),
    sd = as.data.frame(VarCorr(lmm1))$sdcor,
    lo = ci[, 1],
    hi = ci[, 2]
)

figure3 <- eff %>%
    mutate(effect = factor(effect, levels = c( "No Instruction", "Stock Market", "Coin Tossing", "Residuals"))) %>%
    ggplot(aes(x = effect, y = sd, color = effect)) +
    geom_segment(aes(xend = effect, y = lo, yend = hi), alpha = .5, size = 2, show.legend = FALSE) +
    geom_point(shape = 21L, fill = "white", size = 3, show.legend = FALSE) +
    geom_signif(comparisons = list(c("No Instruction", "Stock Market"),
                                   c("No Instruction", "Coin Tossing")),
                color = "black",
                linetype = 2L,
                annotations = c("p = .012", "p =.001"),
                step_increase = .1) +
    
    labs(x = "Conditions", y = "Standard deviation of the Aglorithmic Complexity", color = "")
figure3
figure3 %>%
  ggsave(filename = file.path(FIG,"figure_3.png"),
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 6)
```

```{r table1}
Predictors_names <- rownames(coef(summary(lmm1)))
coef(summary(lmm1)) %>% 
  as.tibble() %>% 
  mutate(Predictors = Predictors_names) %>%
  select(Predictors,
         "Estimates" = 1,
         "Std. Error" = 2,
         "df*"=df,
         "t-value" = 4,
         "p-value" = 5) %>%
  kable(caption = "Model specification. The dependent variable (rolling algorithmic complexity of the sliding window of the length 8) was logged so back-transformed model predictions were bounded to be non-negative since there is no notion of negative randomness / algorithmic complexity.") %>%
  footnote(general = 'Marginal R2 = 2.83%, Conditional R2=46.82% \\nStd. Deviation of the random individual effects s=.048, p<.001 \\n\\*Degrees of Freedom were adjusted with the Kenward-Roger Method')
```

Altoghether, the results of Study 1. supported hypotheses one and three. The task description affected the randomness of human-generated series. People produced more random sequences when the instructions included examples of random processes (imagining tossing a fair coin or imagining a random fluctuation of the stock market chart) than when there was no instruction and participants were duly asked to produce random series. That was because in these conditions, unlike in the No Instruction condition, subjects avoided producing a series of low randomness, hence, the in-between people variance was smaller. Therefore, the examples of random processes in experimental conditions did not serve to produce more random series but inhibited producing ones of low algorithmic complexity. This conclusion might be counter intuitive for many researchers (compare [@ayton1991]) who believed that more relatable instructions facilitated producing series of high randomness. Additionally, Study 1., supported hypothesis three that randomness of the series decreases over time. The prolonged cognitive engagement in mundane and repetitive experimental tasks deteriorated subjects' performance in random series production. However, in this study we modeled this trend using linear estimation, which needs further examination because no notion of negative algorithmic complexity / randomness.

\subsection{Study 2.}

The aim of the second study was to further investigate the observed effect of fatigue on the random series production (H3) and to examine the relationship of the algorithmic complexity with the need for engaging cognitive resources in challenging tasks (H4). In Study 1., the results showed that the algorithmic complexity of the series decreased over time. This effect was caused by fatigue with a task requiring constant attention. Participants in almost 15 minutes-long procedure were asked to produce 300 elements series without any break. In Study 2., we wanted to investigate whether this effect would be also present in series of shorter independent tasks since many researchers [@kareev1995] argued that the reason behind people's inability to produce random-like series lies in the fact that subjects do not perceive each element as an independent. Therefore, in this study, participants were asked to produce 10 independent binary series and to complete Polish versions of the Need for Cognition Scale. Participants were recruited through the Polish Nationwide Opinion Poll Ariadna. It is a Polish Online Opinion Poll often used to conduct political surveys or scientific research. Depending on the declared length of the study users are gratified with points that they can exchange for prizes.

```{r load_study2_data}
data2 <- read.csv2(file.path(DATA, "Study2.csv")) %>%
  mutate(warunek = as.factor(warunek),
         IsHomogenous = if_else(warunek == "homogeneous", 0, 1),
         id = as.factor(id))
```

\subsubsection{Procedure and Design}

The study followed an experimental design, including one two-level between-subjects variable. First, participants were at random assigned to either the homogeneous instruction condition or the heterogeneous instruction condition. In both conditions, similarly to study 1, participants saw a red square every two seconds in ten 12-displays series. In the homogeneous condition for each series, they were asked to imagine tossing a fair coin and report the result whenever they saw a red square. In the heterogeneous condition, their task altered every second series. In the odd series, they were asked to imagine tossing a fair coin and report the result whenever they saw a red square. In the even series, participants were asked to imagine a stock market chart and to assume that price fluctuation is generated by a random process. They were instructed to try to predict whether the prices will go up or down in the next time step and to report the outcome every time they saw a red square. After completing the procedure of generating random series participants in both conditions were asked to fill in Polish version of the Need for Cognition Scale.

\subsubsection{Participants}

The participants were recruited through the Polish Nationwide Opinion Poll Ariadna. A total number of 266 subjects agreed to take part in the study. However, due to the unrealistic (short or long) time of completion and unfinished surveys we excluded 80 participants. Therefore, finally we had a sample of 186 participants (\(134\ females\)), aged from 18 to 77 (\(M = 39.32,\ SD = 13.08\)). They were at random assigned to one of the experimental conditions. The procedure was approved by the ethics committee of the Robert Zajonc Institute for Social Studies at the University of Warsaw. All participants gave informed consent before taking part in the study.

\subsubsection{Data manipulation}

In online research, it is crucial to measure survey time and exclude participants who completed the task in unrealistic (short or long) time. In our case, we removed \(15\%\) of the shortest answers and 15\% of the longest responses. Although the participants were instructed to only press relevant keys when they saw a red square some people pressed it less frequently than 120 times (twelve ten elements long series). Therefore, the total number of the produced elements in all series varied between subjects from 100 to 120 elements (\(Median = 114\)).

We used ''pybdm'' library in Python to compute the algorithmic complexity of each series\footnote{It is available on GitHub under MIT License \url{https://github.com/sztal/pybdm}.}. It is an implementation of the Block Decomposition Method which allows extending the power of the Coding Theorem Method on longer strings [@zenil2018].


For each series produced by participants, we computed its algorithmic complexity. Therefore, each participant was represented by ten data points ordered in a time series. The algorithmic complexity of each series was normalized because although the instruction asked to produce 12 elements binary series the length varied. Normalization of the algorithmic complexity allowed us to compare a series of different lengths.

```{r model_all}
data2$warunekO <- as.ordered(data2$warunek)
contrasts(data2$warunekO) <- "contr.treatment"

bamm1 <- bam(cmx ~ potrzeba_poznania +
               warunekO +
               s(ids, bs = 'tp', k = 10) +
               s(ids, by = warunekO, bs = 'tp', k = 10) +
               s(id, bs = 're') +
               s(ids, id, bs = 're'),
             data = data2)
summary(bamm1)
gamm1 <- gamm(cmx ~ potrzeba_poznania +
               warunekO +
               s(ids, bs = 'tp', k = 10) +
               s(ids, by = warunekO, bs = 'tp', k = 10),
              random = list(id = ~ 1, id = ~ids),
              data = data2)
summary(gamm1$gam)
summary(gamm1$lme)
r.squaredGLMM(gamm1$lme)
```
\subsubsection{Results}

In Study 1., we estimated the small negative trend of algorithmic complexity over time with the linear mixed models' approach. Although the model fitted the data and allowed to interfere about the effect of fatigue on algorithmic complexity it did not account for the fact that there is no notion of negative algorithmic complexity/randomness hence the trend curve must be bounded to zero. Therefore, to analyze the results of Study 2., we used a generalized additive mixed models approach which enabled modeling much more complex trend curves. Under two experimental conditions (heterogeneous and homogeneous conditions) we tested whether altering task descriptions in-between series would diminish the fatigue effect. As the dependent variable in the model, we had normalized algorithmic complexity. Normalization allowed to compare a series of different lengths because although the instruction asked to produce 12 elements binary series the length varied. As parametric effects, we entered the linear difference between trends of algorithmic complexity in homogenous and heterogenous conditions (with the homogeneous condition as the reference level in dummy coding) and the results of the Need for Cognition Scale. As nonparametric effects, we had a non-linear difference in trends of algorithmic complexity over time in-between homogenous and heterogeneous conditions (with the homogeneous condition as the reference level in dummy coding) and non-linear trend of algorithmic complexity over time. As random effects, we entered an intercept for subjects and random effect of slope for the series' number.Goodness-of-fit of the models were assessed with marginal \(R^2\) (variance retained by fixed effects only) and conditional \(R^2\) (variance retained by the model as such).

The fitted model explained $50.7\%$ of the variance, while fixed effects $10.67\%$. The non-linear trend of algorithmic complexity over time was significant (compare Figure 4.), $F(edf=3.587, Ref.df=3.587) = 11.863, p < .0001$. The negative effect of time continued from series number one to series number five and afterward, the curve flattened at approximately the same level from series six to ten. However, neither linear ($t(180) = -1.69, p = .091$) nor non-linear ($F(edf = 1, Ref.df = 1) = 1.956, p = .162$) difference between homogenous and heterogeneous conditions were significant. The alternation of tasks description did not inhibit the effect of fatigue on the algorithmic complexity. Additionally, there was a small significant parametric effect of of Need for Cognition, ($t(180) = 2.66, p = .009$). With $.01$ point increase of normalized algorithmic complexity, there was $.2\ \pm.093$ point change of Need for Cognition scale results. This positive relationship between the randomness of series and the Need for Cognition scale results supported hypothesis four. Although the effect was negligible its direction suggests that people with a tendency to engage in intellectually challenging tasks perform better in the random-series generation tasks.

Although the hypothesis regarding the effect of task alternation on the negative trend of algorithmic complexity was not supported the effect of fatigue between series was non-linear. The observed trend of algorithmic complexity over time (compare Figure 4.) at its beginning was similar to the trend observed in Study 1. However, after series number five the curve flatters. That suggests that although the effect of fatigue has a negative influence it is bounded to 0.5 normalized Algorithmic Complexity. That is because although normalized Algorithmic Complexity varies from zero to one values below 0.5 became highly unlikely. Additionally, the observed correlations between the algorithmic complexity and the Need for Cognition Scale supported hypothesis four. It suggests that the ability to produce random series might be attributed not only to situational determinants (task description like in Study 1.) but also to cognitive motivation. However, the need for cognition is positively correlated with fluid intelligence. On one hand, it is easier to enjoy intellectually challenging tasks when there is a history of successfully resolved ones. On the other hand, engaging in challenging tasks increases the probability of success [@matusz2011}. Therefore, it seems that the intellectual determinants of the ability to produce random series (like fluid intelligence) might be moderated by cognitive motivation. It would explain why the observed effect of need for cognition is of negligible size.

```{r plot4, include = TRUE, fig.cap = "The trend curve of the Normalized Algorithmic Complexity over series' number with 95\% confidence interval. For each series produced by participants, we computed its algorithmic complexity. Therefore, each participant was represented by ten data points ordered in a time series. The algorithmic complexity of each series was normalized because although the instruction asked to produce 12 elements binary series the length varied. Normalization of the algorithmic complexity allowed us to compare a series of different lengths."}
data_plt4 <- get_predictions(gamm1$gam,
                        cond = list(#warunekO = c("heterogeneous", "homogeneous"),
                                    ids = seq(min(data2$ids),
                                              max(data2$ids),
                                              length = 10)),
                        se = TRUE,
                        print.summary = FALSE)

figure4 <- data_plt4 %>%
  ggplot(aes(x = ids, y = fit)) +
  geom_line(aes(color = warunekO), show.legend = FALSE)  +
  geom_ribbon(aes(ymin = fit-CI, ymax = fit + CI, group = warunekO), alpha = .1) +
  scale_x_continuous(breaks=seq(0,10,1))+
  ylab("Normalized Algorithmic Complexity") +
  xlab("Series Number")
figure4 
figure4 %>%
  ggsave(filename = file.path(FIG,"figure_4.png"),
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 6)
```

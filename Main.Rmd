---
title: "XXX"
description: |
    Method section.
author:
  - name: Mikołaj Biesaga and Szymon Talaga
    affiliation: The Robert Zajonc Institute for Social Studies
    affiliation_url: www.iss.uw.edu.pl/en/
output: 
    pdf_document: 
      keep_tex: yes
bibliography: literature.bib
csl: apa-single-spaced.csl

---

\section{Method}

Our goal was to investigate described above hypothesis in two studies. In study 1., in experimental design, we tested hypotheses regarding the effect of task description (H1) and mathematical experience (H2) on the algorithmic complexity of human-generated series. Additionally, in all experimental conditions, we investigated the dynamic of the randomness of human-generated series (H3). In study 2, we further examined the dynamic of randomness in human-generated series (H3) and the relationship between algorithmic complexity of human-generated series with the need for cognition (H4). 

```{r setup_env, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE, message = FALSE, warning = FALSE, fig.width = 8, fig.asp = 1)
```

```{r setup}
## Wczytaj pakiety
library(magrittr)
library(tidyverse)
library(broom)
library(reticulate)
library(lattice)
library(zoo)
library(MuMIn)
library(acss)
library(haven)
library(lme4)
library(lmerTest)
library(kableExtra)
library(ggpubr)
library(quantreg) 
library(emmeans)
source("helpers/helpers.R")
## Ustaw temat ggplot()
theme_set(theme_classic())
COLORS <- RColorBrewer::brewer.pal(8, "Set1")
options(
    ggplot2.discrete.color = COLORS,
    ggplot2.discrete.fill  = COLORS
)
## Środowisko Conda
use_condaenv("bdm")
```

\subsection{Study 1}

In the first study, participants were tested individually in sessions that lasted about 15 minutes. They were simply asked to produce a 300 elements binary series. We gathered the responses from the participants using a specially designed computer tool\footnote{It is available on GitHub under MIT License \url{https://github.com/MikoBie/Survey}.}.

```{r load_and_prepare_data}
## Wczytanie danych z wyniki_new.csv
data <- read_delim("data/Study_1.csv", delim = ";") %>%
    rename_at(vars(matches("^\\d")), ~str_c("d", .x)) %>%
    mutate_at(vars(matches("^d\\d")), as.integer) %>%
    rename(id = X)

## Zamiana na format long
data_long <- gather(data, key = "Index", value = "Bit", matches("^d\\d")) %>%
    filter(!is.na(Bit)) %>%
    arrange(id) %>%
    group_by(id) %>% 
    mutate(idx = 1:n()) %>%
    filter(idx < 313) %>%
    summarize(seq = list(Bit)) %>%
    ungroup 
``` 

\subsubsection{Procedure and Design}

The experiment followed a 2 x 3 factorial design, including two between-subjects variables: the mathematical experience and the task description condition. We recruited as participants two groups of students who either studied Psychology or Chemistry at the University of Warsaw. We assumed that students who chose as their major Psychology had a relatively small experience with concepts like randomness while students from the Chemistry Department were more familiar with them. We based our assumption on the number of obligatory courses students had to take in both departments. For Chemistry it was nearly 200 hours of subjects like Math, Physics, and Statistics during the first three years, while for Psychology it was just 90 hours of Statistics for five years. In both groups, participants were at random assigned to one of three experimental conditions: 1) the No Instruction Condition, 2) the Coin Tossing Condition, and 3) the Stock Market Condition. In all three conditions, we used a specially designed computer tool. It displayed every two seconds a red square. In the No Instruction Condition participants were simply asked to press in random order one of the two specially marked keys whenever they see a red square. In the Coin Tossing Condition, participants were instructed to imagine tossing a fair coin whenever they see a red square and press either key marked as tail or head. In the Stock Market Condition, participants were asked to imagine a stock market chart and to assume that price fluctuation is generated by a random process. They were instructed to try to predict whether the price in the next time step will go up or down and to press either key marked as an arrow up or down.

\subsubsection{Participants}

The participants were students from the Psychology Department and the Chemistry Department at the University of Warsaw. A total of \(183\) subjects (\(129\ females\)), aged from 18 to 30 (\(M = 21.54,\ SD = 2.12\)), were randomly assigned to one of the experimental conditions. The procedure was approved by the ethics committee of the Robert Zajonc Institute for Social Studies at the University of Warsaw. All participants gave informed consent before taking part in the study.

\subsubsection{Data manipulation}

Although the participants were instructed to only press relevant keys when they saw a red square some people pressed it more often and others less frequently than 300 times. Therefore, the length of the series varied between subjects from 218 to 1016 elements (\(Median = 300\)). However, there were only a few people who produced significantly longer series than others. Therefore, we cut off observations exceeding the typical length of the series, i.e.~we cut off the last 10\% of observations (with indexes longer than 313).

We used ''pybdm'' library in Python to compute algorithmic complexity of each series\footnote{It is available on GitHub under MIT License \url{https://github.com/sztal/pybdm}.}. It is an implementation of Block Decomposition Method which allows extending the power of the Coding Theorem Method on longer strings [@zenil2018]. All other analyses were performed using R [@rcore]. We used packages ''dplyr'' [@dplyr], ''nlme'' [@nlme], and ''ggplot2'' [@ggplot2] for data manipulation, processing, visualization, and mixed models computation.

For each participant, we computed the overall value of the series algorithmic complexity and the rolling algorithmic complexity. The former was normalized because even though we cut off 10\% of the longest indexes, the length of series still varied. Normalization of the algorithmic complexity allowed to compare series of different lengths. The rolling algorithmic complexity was based on the computation of algorithmic complexity for a sliding window of length from 5 to 9. That is because the length of the working memory capacity is \(7\ \pm 2\) bits of information [@baddeley1986]. Herein, we present the analysis only for the sliding window of length 8, but other analysis with reproducible R code might be found in Supplementary Materials.

```{python prepare_strings}
import numpy as np
import pandas as pd
from pybdm import BDM
from pybdm import PartitionIgnore, PartitionRecursive

def window_bdm(seq, bdm, k=8):
    return np.array([ bdm.bdm(seq[i:(i+k)]) for i in range(len(seq) - k) ])
    
data = r.data_long
data.id = data.id.astype(int)
data.seq = data.seq.apply(lambda x: np.array(x, dtype=int))

bdm_recursive = BDM(ndim=1, partition=PartitionRecursive, min_length=8)
bdm_ignore = BDM(ndim=1, partition=PartitionIgnore)
seq8 = pd.DataFrame({'id': r.data_long.id.astype(int),
                     'cmx': data.seq.apply(bdm_ignore.nbdm),
                     'cmx_w': data.seq.apply(lambda x: window_bdm(x, bdm_recursive, k = 8))})
```


```{r data_processing}
seq8 <- as_tibble(py$seq8)

data <- select(data, -matches("^d\\d")) %>%
    filter(id %in% seq8$id) %>%
    left_join(select(seq8, id, cmx), by = "id") %>%
    mutate(Condition = case_when(Condition == "coin" ~ "Coin Tossing",
                                 Condition == "stock" ~ "Stock Market",
                                 Condition == "zero" ~ "No Instruction"),
           Condition = factor(Condition, levels = c( "No Instruction", "Stock Market", "Coin Tossing")))
           

seq8 <- seq8 %>%
    unnest(cols = cmx_w) %>%
    mutate(cmx_w = as.vector(cmx_w)) %>%
    group_by(id) %>%
    mutate(idx = 1:n(),
           cmx_r = rollmean(cmx_w, k = 8, align = "center", na.pad = TRUE)) %>%
    ungroup %>%
    left_join(select(data, -cmx), by = "id") %>%
    mutate(Condition = fct_relevel(as.factor(Condition), "No Instruction")) %>%
    filter(idx < 313)  %>%
    mutate(cmx_wc = mean(cmx_w) - cmx_w)
```

\subsubsection{Results}

Before conducting a more detailed analysis, we tested hypotheses regarding the effect of the task description (H1) and mathematical experience (H2) on the overall algorithmic complexity of human-generated series. For testing these hypotheses we used non parametric tests to assess whether the distributions of algorithmic complexity were systematically different between groups of observations.

```{r h1_test}
kruskal.test(cmx ~ Condition, data = data)
```

```{r plot1, include = TRUE, fig.cap = ""}
plt1 <- data %>%
    ggplot(aes(x = cmx, fill = Condition)) +
    geom_histogram(bins = 10, color = "white") +
    facet_wrap(~Condition, ncol = 1L) +
    labs(x = "Normalized Algorithmic Complexity", y = "Count") +
    guides(fill = FALSE)

plt2 <- data %>%
    ggplot(aes(sample = cmx, color = Condition)) +
    geom_qq_line(linetype = 2L, color = "black") +
    geom_qq(show.legend  = FALSE) +
    facet_wrap(~Condition, ncol = 1L) +
    labs(x = "Normal quantiles", y = "Observed quantiles")

figure1 <- ggarrange(plt1,plt2)

figure1

figure1 %>% ggsave(filename = "figure_1.png",
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 6)
```

The Kruskal-Wallis Rank Sum Test revealed a significant difference between distributions of algorithmic complexity in tasks description's experimental conditions, $\chi^2(2) = 7.0051, p < .0301$. The closer visual examination of the plots showed that the main variation between distributions is observed between their lower ends (compare Figure 2.). In the No Instruction condition, the frequency of the lowest results is relatively higher than in the Coin Tossing and the Stock Market conditions. Therefore, to better understand this effect, we performed quantile regression across quantiles of the distributions. As the dependent variable we entered normalized algorithmic complexity and as the fixed effect the task description condition (with No Instruction Condition as a reference level in dummy coding). The analysis revealed that in the first quantile ($\tau = .25$) there was a significant difference in distributions between the Stock Market Condition and the No Instruction Condition, $95%\ CI\ [0.0868\ 0.4576]$ and the Coin Tossing Condition and the No Instruction Condition, $95%\ CI\ [0.0851\ 0.4079]$. In the second quantile ($\tau = .5$), there was significant difference in distributions between Coin Tossing Condition and No Instruction Condition, $95\%\ CI\ [0.0118\ 0.872]$. There were no significant differences in distributions in the third quantile ($\tau = .75$). These results indicate that both the Stock Market and Coin Tossing conditions prevent people from producing series of low algorithmic complexity. The differences in the task description conditions seem not to affect the distributions in its higher ends.

```{r quantreg}
# Full quantile process model
proc <- rq(cmx ~ Condition, tau = -1, data = data, method = "br")

# Quartile model
qreg <- rq(cmx ~ Condition, tau = 1:3/4, data = data, method = "br")
summary(qreg, se = "rank")
```

```{r plot2, include=TRUE, cap.fig=""}
figure2 <- t(proc$sol) %>%
    as_tibble %>%
    select(
        tau,
        zero  = `(Intercept)`,
        coin  = `ConditionCoin Tossing`,
        stock = `ConditionStock Market`
    ) %>%
    mutate(
        "Coin Tossing" = zero + coin,
        "Stock Market" = zero + stock,
        "No Instruction" = zero
    ) %>%
    pivot_longer(-c(tau:stock), names_to = "Condition", values_to = "Qbar") %>%
    mutate(Condition = factor(Condition, levels = c("No Instruction", "Stock Market", "Coin Tossing"))) %>%
    ggplot(aes(x = tau, y = Qbar, color = Condition, group = Condition)) +
    geom_line() +
    geom_vline(xintercept = .5, linetype = 2L) +
    labs(x = "Quantile", y = "Normalized Alogorithmic Complexity")

figure2

figure2 %>% ggsave(filename = "figure_1.png",
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 6)
```

```{r h2_test}
c(
    list(Faculty = data),
    split(data, data$Condition)
)%>%
    map2(names(.), ~{
        mutate(tidy(wilcox.test(cmx ~ Faculty, data = .x, exact = FALSE)), condition = .y, .before = 1L)
    }) %>%
    bind_rows %>%
    select(
        Condition = condition,
        W = statistic,
        p = p.value
    )
```

To test the hypothesis that students with more mathematical experience will produce series more complex than others we compared distributions of algorithmic complexity in groups of chemistry and psychology students. The Wilcoxon Rank Sum Test revealed that the main difference was not siginificant, $W = 4191.5,\ p=.9866$. The further analysis within experimental conditions also did not yield significant differences in distributions of algorithmic complexity between faculties (No Instruction Condition, $W = 451.5,\ p = .8083$; Stock Market Condition, $W = 531,\ .3447$; Coin Tossing Condition, $W = 422,\ p = .3184$). Although this result suggests that the mathematical experience does not affect the overall algorithmic complexity of produced series we would argue that this conclusion needs further examination. In this study, we interfered with mathematical experience based on the field of studies. Therefore, we did not control for the actual mathematical experience but based our assumption on the number of classes students have taken over the course of their academic career.

```{r model8}
lmm0 <- lmer(log(cmx_w) ~  idx + Faculty * Condition + (1 | id), data = seq8)

lmm1 <- lmer(
    formula = log(cmx_w) ~ Condition + idx + Faculty +
        (0 + dummy(Condition, "No Instruction") | id) +
        (0 + dummy(Condition, "Coin Tossing") | id) +
        (0 + dummy(Condition, "Stock Market") | id),
    data = seq8)

summary(lmm1)

compare <- emmeans(lmm1, "Condition", lmerTest.limit = 53164) %>%
    pairs(rev = TRUE, adjust = "holm")

compare
r.squaredGLMM(lmm1)

prof <- profile(lmm1, which = c("theta_"), signames = FALSE)
(ci <- confint(prof))

anova(lmm0, lmm1)
```

In a more detailed analysis, we used the rolling algorithmic complexity. We performed the linear mixed-effects model to test the third hypothesis that the algorithmic complexity decreases over time. We present herein analysis for a sliding window of length 8 only. That is because this model fitted the data the best. The specification of the rest of the models is in Supplementary Materials together with a replicable R script. The dependent variable (the algorithmic complexity) in the models was logged so back-transformed model predictions were bounded to be non-negative since there is no notion of negative randomness / algorithmic complexity. In all models as fixed effects we had the time step, the task description condition (with No Instruction Condition as a reference level in dummy coding), the mathematical experience condition (with Chemistry as a reference level in dummy coding) and interaction terms between the time step and the task description condition, and between the time step and the mathematical experience condition. Goodness-of-fit of the models were assessed with marginal \(R^2\) (variance retained by fixed effects only) and conditional \(R^2\) (variance retained by the model as such).

In the model with the sliding window of length 8 as a random effects, we had an intercept for subjects as well as by-subject random slopes for task description conditions. The fitted model indicated several significant effects. There was a significant negative effect of the time step on the algorithmic complexity, \(t(52978) = -21.98,\ d = .006,\ p < .01\). With each time step, the algorithmic complexity decreased by \(.004\%\ \pm.0002\). This result supported the third hypothesis regarding the observed decline in the algorithmic complexity over time. Additionaly, similarly to the overall algorithmic complexity, there were significant differences between task definition conditions but not between the levels of mathematical experience. The algorithmic complexity in the Coin Tossing Condition was \(2.05\%\ \pm.8\) higher than in the No Instruction Condition, \(t(106) = 2.68,\ d = .38,\ p = .02\). Similarly, in the Stock Market condition, the algorithmic complexity was higher \(1.69\%\ \pm.74\) than in the No Instruction Condition, \(t(179) = 2.51,\ d = .36,\ p = .04\). Although listed fixed effects were statistically significant they explained only $2.85\%$ of the variance while random effects retained $46.82\%$ of the variance. The closer examination revealed that the model with intercept for subjects as well as by-subject random slopes for task description conditions better fitted the data than the model with only random intercept for subjects, $\chi^2= 10.5, p < .001$. Therefore, in-between people variance was the function of the task description condition. The profiled confidence intervals of standard deviation in all three experimental conditions ploted in Figure 4. shows that in No Instruction condition in-between people variance was the highest. This result is in line with quantile regression results. The reason why people perform better in the Coin Tossing Condition and the Stock Market Condition than in the No Instruction Condition lies in the fact that they do not produce series of low complexity.

```{r plot3, include = TRUE, fig.cap = "The decline of the rolling algorithmic complexity over the time step. The grey line depicts the empirical change, while the black line shows value predicted by the model with the (95%) Confidence Interval."}
eff <- tibble(
    effect = fct_inorder(c("No Instruction", "Coin Tossing", "Stock Market", "Residuals")),
    sd = as.data.frame(VarCorr(lmm1))$sdcor,
    lo = ci[, 1],
    hi = ci[, 2]
)

figure3 <- eff %>%
    mutate(effect = factor(effect, levels = c( "No Instruction", "Stock Market", "Coin Tossing", "Residuals"))) %>%
    ggplot(aes(x = effect, y = sd, color = effect)) +
    geom_segment(aes(xend = effect, y = lo, yend = hi), alpha = .5, size = 2, show.legend = FALSE) +
    geom_point(shape = 21L, fill = "white", size = 3, show.legend = FALSE) +
    labs(x = "", y = "Standard deviation", color = "")
figure3
figure3 %>%
  ggsave(filename = "figure_3.png",
       device = "png",
       dpi = 300,
       height = 4.5,
       width = 6)
```

```{r table1}
Predictors_names <- rownames(coef(summary(model8)))
coef(summary(model8)) %>% 
  as.tibble() %>% 
  mutate(Predictors = Predictors_names) %>%
  select(Predictors,
         "Estimates" = 1,
         "Std. Error" = 2,
         "df*"=DF,
         "t-value",
         "p-value") %>%
  kable(caption = "Model specification. The dependent variable (rolling algorithmic complexity of the sliding window of the length 8) was logged so back-transformed model predictions were bounded to be non-negative since there is no notion of negative randomness / algorithmic complexity.") %>%
  footnote(general = 'Marginal R2 = 2.86%, Conditional R2=45.1% \\nStd. Deviation of the random individual effects s=.048, p<.001 \\n\\*Degrees of Freedom were adjusted with the Kenward-Roger Method \\nThe model accounted for first-order auto-regressive errors correlation structure')
```

The results suggest that although there is a significant difference between task definition conditions in average algorithmic complexity it does not affect the effect of the time step on the algorithmic complexity. The decline is constant across all conditions (see Figure 2). Comparing to other models using different lengths of the rolling window this model explained the most variance, both in terms of fixed and random effects. However, it is worth noting that models with the length of the sliding window 5 and 6 did not yield significant effects of the time step and the difference between the No Instruction Condition and the Stock Market Condition. This might be ascribed to the fact that for shorter strings there is a wider variability in the rolling algorithmic complexity measure (see Supplementary Materials).

\subsection{Study 2.}

In the second study, participants were recruited through the Polish Nationwide Opinion Poll Ariadna. It is a Polish Online Opinion Poll often used to conduct political surveys or scientific research. Depending on the declared length of the study users are gratified with points that they can exchange for prizes. During the research, participants were asked to produce 10 twelve elements binary series and to complete Polish versions of the Need for Cognition Scale.

```{r load_and_prepare_data2}
data2 <- read_sav("data/Study_2.sav") %>%
  mutate(id = 1:n()) %>%
  select(-Id) %>%
  filter(survey_finish_time >= 656 & survey_finish_time < 1708) %>%
  rename_at(vars(matches("^v1_r\\d+")),
            ~str_extract(string = .x, pattern = '\\d+$') %>%
              str_c("pzp", .)) %>%
  rename_at(vars(matches("^v2_r\\d+")),
            ~str_extract(string = .x, pattern = '\\d+$') %>%
              str_c('pp', .)) %>%
  mutate_at(vars(matches("pp\\d+")), ~as.numeric(.)) %>%
  mutate_at(vars(matches("pzp\\d+")), ~as.numeric(.))
```

```{r tests_calculations}
data2 <- data2 %>%
  mutate_at(vars(matches("pp[2, 3, 8, 10, 13, 15, 17, 19, 22, 26, 28, 33, 34, 35]")), ~{6-.}) %>%
  mutate_at(vars(matches("pzp[2, 5, 10, 14, 15, 16, 17, 18, 20, 24, 30, 31]")), ~{7-.}) %>%
  mutate(year = as.numeric(year)) %>%
  select(-age) %>%
  rowwise() %>%
  mutate(preferowanie_porzadku = sum(pzp1, pzp6, pzp17, pzp19, pzp26, pzp27, pzp28),
         preferowanie_przewidywalnosci = sum(pzp5, pzp7, pzp9, pzp15, pzp16, pzp21, pzp22, pzp32),
         nietolerowanie_wieloznacznosci = sum(pzp3, pzp8, pzp12, pzp24, pzp25, pzp29),
         zamknietosc_umyslowa = sum(pzp2, pzp4, pzp20, pzp23, pzp30, pzp31),
         zdecydowanie = sum(pzp10, pzp11, pzp13, pzp14, pzp18),
         preferowanie_porzadku2 = sum(pzp6, pzp26, pzp27),
         preferowanie_przewidywalnosci2 = sum(pzp9, pzp21, pzp32),
         nietolerowanie_wieloznacznosci2 = sum(pzp3, pzp8, pzp29),
         zamknietosc_umyslowa2 = sum(pzp2, pzp20, pzp31),
         zdecydowanie2 = sum(pzp13, pzp14, pzp18),
         potrzeba_poznania = sum(pp1, pp2, pp3, pp4, pp5, pp6, pp7, pp8, pp9, pp10,
                                 pp11, pp12, pp13, pp14, pp15, pp16, pp17, pp18, pp19, pp20,
                                 pp21, pp22, pp23, pp24, pp25, pp26, pp27, pp28, pp29, pp30,
                                 pp31, pp32, pp33, pp34, pp35, pp36),
         warunek = as.character(warunek),
         warunek = if_else(warunek == "1", "homogeneous", "heterogeneous")) %>%
  select(-matches('pp\\d+|pzp\\d+'))

data2_time <- data2 %>%
  gather(key = "Index", value = "Bit", matches("war[[:digit:]]")) %>%
  filter(!is.na(Bit)) %>% 
  filter(Bit != 99) %>% 
  mutate(klucz = if_else(grepl(x = Index, pattern = "time"), 'time', 'seq'),
         ids = str_extract(Index, pattern = '^war1_\\d+|^war2g\\d+_|^war2rm\\d+_'),
         ids = if_else(grepl(x = ids, pattern = 'war1'),
                       str_extract(string = ids, pattern = '(\\d+)(?!.*\\d)'),
                       str_extract(string = ids, pattern = '([rmg]+\\d+)(?!.*\\d)')),
         ids = case_when(ids == 'rm1' ~ '1',
                         ids == 'g1' ~ '2',
                         ids == 'rm2' ~ '3',
                         ids == 'g2' ~ '4',
                         ids == 'rm3' ~ '5',
                         ids == 'g3' ~ '6',
                         ids == 'rm4' ~ '7',
                         ids == 'g4' ~ '8',
                         ids == 'rm5' ~ '9',
                         ids == 'g5' ~ '10',
                         TRUE ~ ids),
         ids = as.numeric(ids)) %>%
  select(id, ids, Bit, klucz) %>%
  group_by(id, ids, klucz) %>%
  mutate(idx = 1:n()) %>%
  spread(klucz, Bit) %>%
  filter(!is.na(seq))

data2_long <- data2_time %>%
  select(id, ids, seq) %>%
  group_by(id, ids) %>%
  summarise(seq = list(seq)) %>%
  filter(lengths(seq)>8)
```

\subsubsection{Procedure and Design}

The study followed an experimental design, including one two-level between-subjects variable. First, participants were at random assigned to either the homogeneous instruction condition or the heterogeneous instruction condition. In both conditions, similarly to study 1, participants saw a red square every two seconds in ten 12-displays series. In the homogeneous condition for each series, they were asked to imagine tossing a fair coin and report the result whenever they saw a red square. In the heterogeneous condition, their task altered every second series. In the odd series, they were asked to imagine tossing a fair coin and report the result whenever they saw a red square. In the even series, participants were asked to imagine a stock market chart and to assume that price fluctuation is generated by a random process. They were instructed to try to predict whether the prices will go up or down in the next time step and to report the outcome every time they saw a red square. After completing the procedure of generating random series participants in both conditions were asked to fill in Polish version of the Need for Cognition Scale.

\subsubsection{Participants}

The participants were recruited through the Polish Nationwide Opinion Poll Ariadna. A total number of 266 subjects agreed to take part in the study. However, due to the unrealistic (short or long) time of completion and unfinished surveys we excluded 80 participants. Therefore, finally we had a sample of 186 participants (\(134\ females\)), aged from 18 to 77 (\(M = 39.32,\ SD = 13.08\)). They were at random assigned to one of the experimental conditions. The procedure was approved by the ethics committee of the Robert Zajonc Institute for Social Studies at the University of Warsaw. All participants gave informed consent before taking part in the study.

\subsubsection{Data manipulation}

In online research, it is crucial to measure survey time and exclude participants who completed the task in unrealistic (short or long) time. In our case, we removed \(15\%\) of the shortest answers and 15\% of the longest. Although the participants were instructed to only press relevant keys when they saw a red square some people pressed it less frequent than 120 times. Therefore, the length of the series varied between subjects from 100 to 120 elements (\(Median = 114\)).

Additionally, we removed \(10\%\) of the shortest series (shorter than 9 elements). That was because we wanted to also be able to compute algorithmic complexity for the sliding window of lengths from 5 to 9.

We used ''pybdm'' library in Python to compute algorithmic complexity of each series\footnote{It is available on GitHub under MIT License \url{https://github.com/sztal/pybdm}.}. It is an implementation of Block Decomposition Method which allows extending the power of the Coding Theorem Method on longer strings [@zenil2018]. All other analyses were performed using R [@rcore]. We used packages ''dplyr'' [@dplyr], ''nlme'' [@nlme], and ''ggplot2'' [@ggplot2] for data manipulation, processing, visualization, and mixed models computation.


For each participant, we computed the overall value of the series algorithmic complexity and the rolling algorithmic complexity. The former was normalized because even though we cut off series shorter than 9 elements, the length of series still varied. Normalization of the algorithmic complexity allowed to compare series of different lengths. The rolling algorithmic complexity was based on the computation of algorithmic complexity for a sliding window of length from 5 to 9. That is because the length of the working memory capacity is \(7\ \pm2\) bits of information [@baddeley1986]. We calculated algorithmic complexity for all the lengths separately and performed the analysis for all the lengths. However, even the best-fitted model did not yield any significant effects. Therefore, the analysis for models for a sliding window of length from 5 to 9 are only to be found in Supplementary Materials.

```{python prepare_strings2}
import numpy as np
import pandas as pd
from pybdm import BDM
from pybdm import PartitionIgnore, PartitionRecursive
    
data = r.data2_long
data.id = data.id.astype(int)
data.ids = data.ids.astype(int)
data.seq = data.seq.apply(lambda x: np.array(x, dtype=int))

bdm = BDM(ndim=1, partition=PartitionRecursive)
seq = pd.DataFrame({'id': r.data2_long.id.astype(int),
                     'ids': r.data2_long.ids.astype(int),
                     'cmx': data.seq.apply(bdm.nbdm)})
```


```{r prepare_data_for_models}
seq <- py$seq


data2 <- data2_long %>%
  select(id, ids) %>%
  left_join(data2) %>%
  left_join(seq %>% select(id, ids,cmx)) %>%
  select(-matches('war\\d.')) %>%
  left_join(data2_time %>% select(id, ids, idx, time) %>% group_by(id,ids) %>% summarise(time = sum(time)))
  
```

```{r model_all}
model_quadratic <- lme((cmx) ~ 1 +ids*warunek + I(ids^2)*warunek +
               #preferowanie_porzadku2 +
               #preferowanie_przewidywalnosci2 +
               #nietolerowanie_wieloznacznosci2 +
               #zamknietosc_umyslowa2 +
               potrzeba_poznania,
             random = ~ids|id,
             data = data2,
             correlation = corCAR1(form = ~ids|id),
             method = "ML")
summary(model_quadratic)
r.squaredGLMM(model_quadratic)
d_cohen(model_quadratic)
```
\subsubsection{Results}

First, we computed algorithmic complexity for each series produced by participants. Afterward, we used the linear mixed-effects model to test following hypothesis: H3) the algorithmic complexity decreases over time, H3c) the instruction condition moderates the effect of time on the algorithmic complexity, H4) the need for cognition increases the algorithmic complexity. The dependent variable (the algorithmic complexity) in the model was normalized because even though people were asked to produce a series of 12 elements, the length of the produced series varied. Normalization of the algorithmic complexity allowed to compare a series of different lengths. As fixed effects we entered the instruction condition (with the homogeneous condition as the reference level in dummy coding), the series number (we entered also a quadratic term for the series number because it increased the model fit), the interaction term between series number and the instruction condition, and the need for cognition scale. Goodness-of-fit of the model was assessed with marginal \(R^2\) (variance retained by fixed effects only) and conditional \(R^2\) (variance retained by the model as such). As random effects, we had an intercept for subjects and random effect of slope for the series number.

The fitted model indicated several significant effects. There was a significant negative effect of the series number on the algorithmic complexity, \(t(1501)=4.396,\ d = .13,\ p<.001\). With each series, the algorithmic complexity decreased by \(.045\ \pm.01\) points. However, since the quadratic term was also present, \(t(1501)=3.826,\ d = .009,\ p<.001\), the linear decline of the algorithmic complexity was reduced by \(.003\ \pm.0009\) point with each series. This result supported the third hypothesis regarding the observed decline in the algorithmic complexity over time (compare Figure 3.). However, hypothesis three did not account for the quadratic decrease of the algorithmic complexity decline with each series. It seems that the algorithmic complexity at some point even increased. This change in the trend requires further examination. Hypothesis 3c was not supported. The instruction manipulation affected neither the change of the algorithmic complexity over series nor the algorithmic complexity itself.

The observed trend herein was similar to the trend observed in Study 1. The algorithmic complexity decreased with each series. However, the expected effect of instruction was not present. The decline of the algorithmic complexity was the same in both conditions. Unlike, Study 1 the negative relationship between a series number and algorithmic complexity was quadratic. Therefore, the trend diminished with each series. This result might be attributed either to the difference between tasks in Study 1 and Study 2 or to the different experimental settings (online research vs classic research). Nevertheless, it requires further investigation.

```{r plot3, include = TRUE, fig.cap = "The scatter plot of algorithmic complexity against the series number. The points were jittered for the sake of clearer visualization. The black line depicts the fitted trend of decline of the normalized algorithmic complexity across the series."}
figure3 <- data2 %>%
  ungroup() %>%
  mutate(predict = predict(model_quadratic),
         ymin = predict - 1.96*predict(model_quadratic, se.fit = TRUE, level = 0)$se.fit,
         ymax = predict + 1.96*predict(model_quadratic, se.fit = TRUE, level = 0)$se.fit) %>%
  group_by(ids) %>%
  summarise(predict = mean(predict),
            cmx = mean(cmx),
            ymin = mean(ymin),
            ymax = mean(ymax)) %>%
  mutate(ids = (ids)) %>%
  ggplot(aes(x = (ids), y = predict)) +
  geom_smooth(aes(ymin = ymin, ymax=ymax), stat = 'identity', inherit.aes = TRUE, color = 'black', size = .5) +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10)) +
  geom_jitter(data = data2 %>% ungroup, aes(x = (ids), y = cmx), alpha = .2, color = 'grey') +
  labs(title = "",
       x = "Series number",
       y = "Algorithmic Complexity") 
figure3
#figure3 %>%
#  ggsave(filename = "figure_3.png",
#       device = "png",
#       dpi = 300,
#       height = 4.5,
#       width = 6)
```

Additionally, the fitted model allowed testing hypothesis regarding the Need for Cognition Scale. The Need for Cognition was possessively correlated with the average algorithmic complexity of the series, \(t(179) = 2.66,\ d = .011,\ p < .01\). With the one-point increase on the Need for Cognition scale, there was \(.003\ \pm.001\) increase in the algorithmic complexity. This result supported the hypothesis that people who are more inclined to challenging tasks produces more complex series.

The observed herein correlations allow to assume that the ability to produce random series might be attributed not only to situational determinants but also to cognitive motivation. The relationship between the algorithmic complexity and the need for cognition requires further investigation. It seems that the intellectual determinants of the ability to produce random series (like fluid intelligence) might be moderated by this relationship. Although the presented herein effect is interesting from the theoretical point of view it had negligible effects sizes. Therefore, it requires further examination using statistical tests with higher statistical power.

```{r table2}
Predictors_names <- rownames(coef(summary(model_quadratic)))
coef(summary(model_quadratic)) %>% 
  as.tibble() %>% 
  mutate(Predictors = Predictors_names) %>%
  select(Predictors,
         "Estimates" = 1,
         "Std. Error" = 2,
         "df*"=DF,
         "t-value",
         "p-value") %>%
  kable(caption = "Model specification. The dependent variable was logged so back-transformed model predictions were bounded to be non-negative since there is no notion of negative randomness / algorithmic complexity.") %>%
  footnote(general = 'Marginal R2 = 6.68%, Conditional R2=45.21% \\nStd. Deviation of the random individual effects s=.015, p<.001 \\n\\*Degrees of Freedom were adjusted with the Kenward-Roger Method \\nThe model accounted for first-order auto-regressive errors correlation structure')
```
\section{References}